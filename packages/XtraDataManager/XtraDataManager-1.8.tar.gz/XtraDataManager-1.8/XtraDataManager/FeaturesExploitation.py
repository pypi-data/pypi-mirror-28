# -*- coding utf-8 -*-
from __future__ import print_function
# This script allows to use a scikit-learn.org classifier (supervised or unsupervised),
# in order to sort kilosort/phy generated units according to their cell type
# using their features, extracted thanks to the DataManager class written in FeaturesExtraction.py.

# Maxime Beau, 2017-05-10

import os, os.path, sys
import time

# from XtraDataManager import FeaturesExtraction as fext

# For debugging purpose
sys.path.append("/Users/maximebeau/Desktop/Science/5_Master_2/Internship/Data_Analysis/DataAnalRoutines/XtraDataManager/XtraDataManager")
import FeaturesExtraction as fext

import numpy as np
import itertools

import pandas as pd
from pandas.tools.plotting import table
import matplotlib
matplotlib.use('TkAgg')
from matplotlib import pyplot as plt
from pylab import get_current_fig_manager
from mpl_toolkits.mplot3d import Axes3D
#matplotlib.style.use('fivethirtyeight')
#matplotlib.style.use('ggplot')
#matplotlib.style.use('classic')
matplotlib.style.use('dark_background')

import sklearn
from sklearn.cluster import KMeans, SpectralClustering
from sklearn.preprocessing import StandardScaler, Normalizer, Imputer
from sklearn.pipeline import make_pipeline
from sklearn import datasets, svm, metrics
from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors
from sklearn.model_selection import train_test_split



class CellTypeFinder():
    '''
    The standard way to use CellTypeFinder() is to dial 

    >>> from XtraDataManager import FeaturesExploitation as fexp
    >>> ctf = fexp.CellTypeFinder('directory_with_phy_output')
    >>> data = ctf.data

    - Thus you have access to all your units features through 
    data.MFR, data,IFR, data.CCG, data.ISI, data.WVF - they can be plotted 
    in 'directory_with_phy_output' with >>> data.visualize() + following instructions on the terminal.

    - And you can use the built-in clustering algorithm (classifying to come soon),
    through >>> inspect_labels(self, featuresList=None, unitsList="all", algo=["Clustering", "K-means"], 
                    standardize=True, normalize=True, again=False, plot_crossTab=True, plot_scatter_matrix=True) + following instructions on terminal.
    
    Alternatively, you can only perform your clustering without 
    through >>> ctf.clustering(unitsList='all', featuresList=None, algo="K-means", n_clust_range=[4,5,6,7,8,9,10,11,12,13],
                    algoParams={'n_clusters':10}, 
                    standardize=True, normalize=True, again=False).
    Then the labels are available in the pandas dataframe ctf.clusteringFeaturesDF.
    '''
    
    def __init__(self, directory=None, featuresList=['MFR','CCG', 'ISI', 'WVF'],  bin_sizeCCG=0.0002, window_sizeCCG=0.1, bin_sizeISI=0.0005):
        if directory==None:
            directory = os.getcwd()
        elif directory==1:
            directory='/Volumes/DK_students1/2017-04-08'
        elif directory==2:
            directory='/Users/maximebeau/Desktop/Science/5_Master_2/Internship/Data_Analysis/debugCTC'
        print("\n>>> Extraction of the dataset features:")
        self.data = fext.DataManager(directory)
        self.data.extractFeatures(featuresList=['MFR','CCG', 'ISI', 'WVF'],  bin_sizeCCG=bin_sizeCCG, window_sizeCCG=window_sizeCCG, bin_sizeISI=0.0005)
    
    def preProcessFeatures(self, standardize=True, normalize=True):
        
        # Impute the missing data points -- NOT NEEDED, UNITS WITH NAN VALUES EXCLUDED BEFOREHAND
        #imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
        #imp.fit(self.data.extractedFeatures)
        #self.imputedFeatures = imp.transform(self.data.extractedFeatures)

        # Standardization: mean removal + variance scaling (divide by standard deviation)
        if standardize==True:
            scaler = StandardScaler()
        # Normalizing
        if normalize==True:
            normalizer=Normalizer()

        # Make the resulting pipeline
        if standardize==True and normalize==False:
            pipeline = make_pipeline(scaler)
        elif standardize==False and normalize==True:
            pipeline = make_pipeline(normalizer)
        elif standardize==True and normalize==True:
            pipeline = make_pipeline(scaler, normalizer)

        # Fit the pipeline to the model + Return the data processed through the fitted pipeline
        self.preProcessedFeatures = pipeline.fit_transform(self.data.extractedFeatures)
        
        return self.preProcessedFeatures

    def attribute_humanLabels(self, visualize=True, n_humanLabels=None, pushLoadFile=False, askAgain=True, clustering=True, crossTabProcess=False):
        import itertools
        while 1:
            print('''\n\nThe XtraDataManager.CellTypeFinder tool "attribute_humanLabels" has been called.
            This tool allows you to create a list of cell types ("labels") attributed to your units by eye.

            -> Step 0) This step is not mandatory, and is only performed if the "clustering" argument is set to 
            True when attribute_humanLabels is called.
            A clustering step will be performed. Your units will be clustered in N clusters according to their 
            functional features such as their mean firing rate, the shape of their autocorrelogram or the 
            parameters of their interspike interval distribution. In cunjunction with the display of key functional
            features of the units, this will provide you an additional layer of information 
            to help you with the attribution of cell types to units during Step 2.

            -> Step 1) First of all, you will have to provide a list of cell type names.
            For instance, if your recording is made in the cerebellum cortex and deep 
            nuclei, you will have to dial one by one 'GrC', 'PKJ', 'MLI', 'Golgi', 'DCN' etc.

            -> Step 2) Then, you will iterate through the units and have to pick one of the provided 
            cell types for each of them. This is why we advise you to add a few 'otherx' cell types 
            to your list above: some units activity may be ambiguous, or different from what you expect 
            and thus not sortable as regular cell types with high reliability.

            During the iteration, you will be guided by plots of features generated along the way: the mean 
            firing rate, instataneous firing rate, autocorrelograms, interspike intervals and waveforms will 
            be shown to help your decision. You may also use in parallel other sources of information, 
            by checking your units activity profile on the phy template-gui (https://github.com/kwikteam), 
            or results from optogenetic stimulation of units to identify the cell types.

            Finally, these labels will be added to the units features dataframe self.data.extractedFeaturesDF, 
            callable with self.data.extractedFeaturesDF['cellTypes']. This data frame contains the units features
            extracted with self.data.extractFeatures() used for the clustering, the clustering labels ['Labels'],
            and the attributed cell types ['cellTypes']. It is saved under the name '/humanLabels_X_DataFrame.csv'
            in your data's directory. You can explore it with any excel-like software.

            ADVICE : call attribute_humanLabels a first time to attribute cell types in an uncertain way, and then
            a second time to attribute them in a more certain way, then check behavioral correlations and attribute them
            in an even more certain way.
            In practice : - First round attribute "CellType_sure" and "CellType_Unsure" cell types to your units 
                            ('PKJ+++' and 'PKJ~' for instance).
                          - Second round use the "CellType_Sure" units clustering to help the "CellType_Unsure"
                            units atribution (use the DataFrame automatically saved in /XtraDataManager folder,
                            where your last cell type attributions are saved.).
                          - Third round check behavioral correlations and refine your attributions.


            <- This tool can be called in three situations:
            - You called the method ctf.attribute_humanLabels() by itself to attribute cell types to your units.
            - You tried to perform a clustering, and asked for a cross tabulation to assess its performance: the clustering labels will 
            be compared to the labels you will give yourself right now.
            - You tried to perform classification, and the algorithm needs labeled data to be trained first.\n\n
            

            Generally, if you want to do anything a second time (clustering...), 
            add the argument again=True when you call the function.''')
            
            if not input("Read carefully the instructions above. When you are done, dial enter."):
                break
            else:
                break

        actualname = "/humanLabels_0.npy"
        flag_labelAgain = False

        try:
            # Check if there is at least one human Labels file
            cellTypes = np.load(self.data.__dir__+"/XtraDataManager"+actualname) # If not go to except

            # If no error raised, find the last humanLabels file which was created
            c = itertools.count()
            nextCval = next(c) # next(c)=0
            while os.path.exists(self.data.__dir__+"/XtraDataManager/humanLabels_{}.npy".format(nextCval+1)): # If humanLabels_x+1 exists
                nextCval = next(c)
                actualname = "/humanLabels_{}.npy".format(nextCval) # Actual name is humanLabels_x+1

            # Load corresponding dataframe
            cellTypesOldDF = pd.read_csv(self.data.__dir__+"/XtraDataManager"+actualname[:-4]+"_DataFrame.csv")

            EXIT=False
            while 1:
                if EXIT==True:
                    break
                if pushLoadFile == False:
                    if askAgain==True:
                        labelAgain = input("\n\n\n\nBefore Step 0 or 1: Labels have already been attributed to units by a human. Do you want to attribute labels again ? <y> yes, <n> no: ")
                    else:
                        labelAgain = 'y'
                else:
                    labelAgain = 'n'
                
                if labelAgain=='y':
                    flag_labelAgain = True
                    while 1:
                        ov_cr = input("Do you want to overwrite the last file: \"{}\" or to create a new one ? <ov> overwrite, <cr> create: ".format(actualname))
                        if ov_cr=='ov':
                            print("Ok. {} will be overwritten.".format(actualname))
                            # Actual name is humanLabels_x+1 --> overwrites
                            raise # Go to except
                        elif ov_cr=='cr':
                            # Add a number
                            actualname = "/humanLabels_{}.npy".format(next(c)) # Actual name is humanLabels_x+2 --> newfile
                            print("Ok. {} will be created.".format(actualname))
                            raise # Go to except
                        else:
                            print("Warning: please write either 'cr' or 'ov'.")
                    break

                elif labelAgain=='n':
                    newName = "/humanLabels_0.npy"
                    filesList = []
                    counts = itertools.count()
                    next(counts) # c=0
                    while os.path.exists(self.data.__dir__+"/XtraDataManager"+newName): # If humanLabels_x exists
                        filesList.append(newName) 
                        newName = "/humanLabels_{}.npy".format(next(counts))
                    print("No new human labeling of the units. Which file do you want to use to load the already attributed human labels ?")
                    while 1:
                        print(('- {}\n'*len(filesList)).format(*filesList))
                        fileToLoad = input("Pick a file from above: ")
                        if fileToLoad in filesList:   
                            # cellTypes has already been loaded as a np array above
                            try:
                                self.clusteringFeaturesDF = self.clusteringFeaturesDF.assign(cellTypes=cellTypes)
                                print("{} has been loaded and assigned to the clustering results dataFrame.".format(fileToLoad))
                            except:
                                pass
                            try:
                                self.classifyingFeaturesDF = self.classifyingFeaturesDF.assign(cellTypes=cellTypes)
                                print("{} has been loaded and assigned to the classifying results dataFrame.".format(fileToLoad))
                            except:
                                print("No clustering or classifying results dataFrames found.")
                                pass
                            EXIT=True
                            break
                        else:
                            print("Warning: there is a typo in the file name you provided.")
                else:
                    print("Warning: please write either 'y' or 'n'.")

        except:

            self.possibleCellTypes = []

            if clustering==True:
                print("\n\n\n\nStep 0: clustering of units. CellTypeFinder.clustering() function is called.\n\n")
                self.clustering()

            while 1:
                print("\n\n\n\nStep 1: provide a list of cell types. We advise you to add to your list 3 or 4 'otherx' or 'unknownx' cell type names, to store units you want to group together without actually being sure of their cell type.")
                if crossTabProcess==True:
                    print("/!\ You must provide {} cell types, to be able to compare your labels to the clustering automatically attributed labels.".format(n_humanLabels))

                while 1:
                    if crossTabProcess==True:
                        if len(self.possibleCellTypes) == n_humanLabels:
                            print("\nYou already provided {} cell types - it fits the number of labels of the clustering you provided.".format(len(self.possibleCellTypes)))
                            break
                    possibleCellType = input("\nCell type number {} (dial <d> if you are done): ".format(len(self.possibleCellTypes)+1))
                    if possibleCellType=='d':
                        break
                    else:
                        if possibleCellType in self.possibleCellTypes:
                            print("Cell type already provided.")
                        else:
                            self.possibleCellTypes.append(possibleCellType)
                    print("--> Cell types so far provided: {}".format(self.possibleCellTypes))
                if not input("If you are satisfied, press <enter>. If you want to provide an other list of cell types, dial <other>."):
                    print("\n\n --->>> Cell types ultimately provided: {}. Now you have to attribute units one of these {} clusters/classes.".format(self.possibleCellTypes, len(self.possibleCellTypes)))
                    break
                else:
                    self.possibleCellTypes = []


            self.cellTypes = np.array([])
            self.cellTypesIdx = np.array([])
            print("\n\n\n\nStep 2: Iteration through the units. Gather as much information as you can to label them properly.\n\n")
            for i, unit in enumerate(self.data.goodUnits):
                EXIT=False
                while 1:
                    if EXIT==True:
                        break
                    print("\n\n\n\n||| Label >>>{}<<< by picking one of these cell types: {}. Unit {} out of {}.\n||| First visualize {} features in the popping up windows, then quit them and dial your choice.".format(unit, self.possibleCellTypes, i+1, len(self.data.goodUnits), unit))
                    
                    try:
                        print('''|||| Functional features of unit {}:
|||| - Mean Firing Rate : {}
|||| - 1/autocorrelogram peak dt : {}
|||| - Area under the curve of the 1st fifth of autocorrelogram/total area under the curve of autocorrelogram : {}
|||| - AUC of the 2nd fifth of ACG/total AUC of ACG : {}
|||| - AUC of the 3rd fifth of ACG/total AUC of ACG : {}
|||| - AUC of the 4th fifth of ACG/total AUC of ACG : {}
|||| - AUC of the 5th fifth of ACG/total AUC of ACG : {}
|||| - Mean of interspike intervals distribution : {}
|||| - Variance of ISI distribution : {}
|||| - Skewness of ISI distribution : {}'''.format(self.data.goodUnits[i], *self.data.extractedFeatures[i]))
                        clusterLabel = self.clusteringFeaturesDF["Labels"][unit]
                        print("|||| Label attributed by clustering THIS TIME: {} out of {} clusters.".format(clusterLabel, len(np.unique(self.clusteringFeaturesDF["Labels"]))))
                        clusterLabel_cellTypesList = []
                        for rank, label in enumerate(self.clusteringFeaturesDF["Labels"]):
                            if rank<i and label==clusterLabel: # We are currently screening units and are now at unit #i -> any rank >= i will be out of range of the list celTypes
                                clusterLabel_cellTypesList.append(self.cellTypes[rank])
                        print("|||| Cell types already attributed to units in this cluster SO FAR THIS TIME: {}".format(clusterLabel_cellTypesList))
                    
                        if flag_labelAgain==True:
                            clusterLabel = cellTypesOldDF["Labels"][unit]
                            print("|||| Label attributed by clustering LAST TIME: {} out of {} clusters.".format(clusterLabel, len(np.unique(cellTypesOldDF["Labels"]))))
                            clusterLabel_cellTypesList = []
                            for rank, label in enumerate(cellTypesOldDF["Labels"]):
                                if label==clusterLabel: # We are currently screening units and are now at unit #i -> any rank >= i will be out of range of the list celTypes
                                    clusterLabel_cellTypesList.append(cellTypesOldDF["cellTypes"].tolist()[rank])
                            print("|||| Cell types already attributed to units in this cluster LAST TIME: {}".format(clusterLabel_cellTypesList))
                    except:
                        pass
                    # When classifying will be available
                    # try:
                    #     automaticLabel = self.classifyingFeaturesDF["Labels"][i]
                    #     print("||| Label attributed by classification: {} out of {} classes.".format(automaticLabel, len(np.unique(self.classifyingFeaturesDF["Labels"]))))
                    # except:
                    #     pass
                    
                    if visualize==True:
                        sys.stdout = open(os.devnull, "w")
                        self.data.visualize(unitsList=[unit], featuresList=['MFR', 'CCG', 'ISI', 'WVF'], showMode='all', saveMode=False)
                        sys.stdout = sys.__stdout__
                    
                    while 1:
                        cellType = input("||| --->>> Dial your choice of cell type, or dial <again> if you need to visualize {} features an other time: ".format(unit))
                        if cellType in self.possibleCellTypes:
                            idx = np.where(cellType==np.array(self.possibleCellTypes)[:])[0][0]
                            self.cellTypesIdx = np.append(self.cellTypes, np.array([idx])) # Stored as a number, algorithms can't handle non numerical data
                            self.cellTypes = np.append(self.cellTypes, np.array([cellType]))
                            print("||| You picked {} as a cell type, whose index is {}.\n\n\n\n".format(cellType, idx))
                            EXIT=True
                            break
                        elif cellType == 'again':
                            # No EXIT==True
                            break
                        else:
                            print("||| Heads up, you must pick a cell type from the list you just provided: {}".format(self.possibleCellTypes))

            
            # cellTypes is a np array
            try:
                if crossTabProcess==True:
                    assert self.cellTypes.shape[0] == self.clusteringFeaturesDF.shape[0]
                np.save(self.data.__dir__+"/XtraDataManager"+actualname, self.cellTypes)
                print("{} saved at {}.".format(actualname, self.data.__dir__+"/XtraDataManager"))
                self.data.extractedFeaturesDF = self.data.extractedFeaturesDF.assign(cellTypes=self.cellTypes)
                self.data.extractedFeaturesDF.to_csv(self.data.__dir__+"/XtraDataManager"+actualname[:-4]+"_DataFrame.csv")
            except:
                pass

            # try:
            #     assert self.cellTypes.shape[0] == self.classifyingFeaturesDF.shape[0]
            #     np.save(self.data.__dir__+"/XtraDataManager"+actualname, cellTypes)
            #     print("{} saved at {}.".format(actualname, self.data.__dir__+"/XtraDataManager"))
            #     self.classifyingFeaturesDF = self.classifyingFeaturesDF.assign(cellTypes=cellTypes)
            # except:
            #     pass

        # return self.clusteringFeaturesDF


    def clustering(self, unitsList='all', featuresList=None, algo="K-means", n_clust_range=[4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],algoParams={'n_clusters':10}, standardize=True, normalize=True, again=False):
        '''Unsupervised learning to sort according to cell type.
        Different algorithms 'algo' are available:
        - 'Kmeans'. Set of parameters to enter: 'algoParams'={'n_clusters':10, 'init':'k-means++', 'n_init':10, 'max_iter':300, 'tol':0.0001, 'precompute_distances':'auto', 'verbose':0, 'random_state':None, 'copy_x':True, 'n_jobs':1, 'algorithm':'auto'}
        - 'SpectralClustering'. Set of parameters to enter: 'algoParams'={'n_clusters':8, 'eigen_solver':None, 'random_state':None, 'n_init':10, 'gamma':1.0, 'affinity':'rbf', 'n_neighbors':10, 'eigen_tol':0.0, 'assign_labels':'kmeans', 'degree':3, 'coef0':1, 'kernel_params':None, 'n_jobs':1}
        - 'Nearest-Neighbors'. Set of parameters to enter: 'algoParams'={'n_neighbors':5, 'radius':1.0, 'algorithm':'auto', 'leaf_size':30, 'metric':'minkowski', 'p':2, 'metric_params':None, 'n_jobs':1}'''
        

        try:
            print("Units already clustered.\n", self.clusteringFeaturesDF)
            if again==True:
                if input(" -- Cluster again? Dial <anything> for yes, <enter> for no: "):
                    raise

        except:
            ## Preprocess data if required
            if standardize==True or normalize==True:
                self.preProcessFeatures(standardize=standardize, normalize=normalize)


            ## Select features to use
            allfeaturesList = ["MFRf0", "CCGf0", "CCGf1", "CCGf2", "CCGf3", "CCGf4", "CCGf5", "ISIf0", "ISIf1", "ISIf2"]  
            if (featuresList == None or featuresList == []):
                featuresList = []
                
                while 1:
                    idx = input("\n\nPlease dial a feature to use for the clustering - MFRf0, CCGf0-5, ISIf0-2 or <all> for all features; dial <d> if you are done: ")
                    if idx == "all":
                        featuresList = allfeaturesList
                        break
                    elif idx == "d":
                        break
                    elif idx in allfeaturesList:
                        featuresList.append(idx)
                    else:
                        print("\nYou must dial <MFRf0>, <CCGf0>, <CCGf1>, <CCGf2>, <CCGf3>, <CCGf4>, <CCGf5>, <ISIf0>, <ISIf1> or <ISIf2>.")
            
                if featuresList==[]:
                    print("\nYou didn't provide any feature. All features will then be used.")
                    featuresList = allfeaturesList

            if unitsList=="all":
                allunits=True
                unitsList=self.data.usedUnits   

            print("\n\n--> Units to cluster: ", "all" if allunits==True else unitsList, "\n\n--> Features used for clustering: ", featuresList, "\n\n")
        
            if standardize==True or normalize==True:
                featuresDataframe = pd.DataFrame(data=self.preProcessedFeatures, index=self.data.usedUnits, columns = allfeaturesList)
            else:
                featuresDataframe = pd.DataFrame(data=self.data.extractedFeatures, index=self.data.usedUnits, columns = allfeaturesList)
            self.clusteringFeaturesDF = featuresDataframe.filter(featuresList, axis=1)
            featuresMatrix = self.clusteringFeaturesDF.as_matrix()



            ## Clustering

            #First, pick the best number of cluster with inertia as readout
            inertias = []
            for N_CLUSTERS in n_clust_range:
                algoParams['n_clusters']=N_CLUSTERS
                if algo=="K-means":
                    model = KMeans(**algoParams)       
                if algo=="SpectralClustering":
                    model = SpectralClustering(**algoParams)
                
                model.fit(featuresMatrix)    
                inertias.append(model.inertia_)

            showPlot=True
            while 1:
                if showPlot==True:
                    fig = plt.figure()
                    ax = fig.add_subplot(111)
                    ax.plot(n_clust_range, inertias, '-o')
                    ax.set_xlabel('n_clusters', fontsize=12)
                    ax.set_ylabel('inertia (Sum of distances of samples to their closest cluster centroid.)', fontsize=8)
                    fig.suptitle('Choose n_clusters at the point where the inertia profile presents an elbow', fontsize=12, fontweight='bold')
                    ax.set_title('Aims the lowest inertia possible while having a reasonably low number of clusters.', fontsize=8)
                    fig.canvas.manager.window.attributes('-topmost', 1)
                    plt.show()
                    showPlot=False

                try:
                    BEST_N_CLUSTERS = int(input("Pick the best n_clusters, i.e. the elbow of the inertias graph, or <0> to see the plot again: "))
                    if BEST_N_CLUSTERS == 0:
                        showPlot=True
                    elif BEST_N_CLUSTERS in n_clust_range:
                        print("You picked <", BEST_N_CLUSTERS, "> as n_clusters.")
                        algoParams['n_clusters']=BEST_N_CLUSTERS
                        break
                    else:
                        print("You must dial one of the tested n_clusters, in this list:", n_clust_range)
                except:
                    print("\nWarning: you must dial an integer.")

            # Pick a model, with your n_cluster as argument

            if algo=="K-means" or algo=="K-Means" or algo=="k-means":
                try:
                    model = KMeans(**algoParams)
                except:
                    print('''Woops, looks like you didn't provide a set of parameters 'algoParams' suitable for the clustering algorithm you intent to use, 'K-means'.
                        'algoParams' must be a dictionnary containing at least one of these items:
                        {'n_clusters':10, 'init':'k-means++', 'n_init':10, 'max_iter':300, 'tol':0.0001, 'precompute_distances':'auto', 'verbose':0, 'random_state':None, 'copy_x':True, 'n_jobs':1, 'algorithm':'auto'}''')
            elif algo=="SpectralClustering" or algo=="Spectralclustering" or algo=="cpectralclustering" or algo=="Spectral-Clustering":
                try:
                    model = SpectralClustering(**algoParams)
                except:
                    print('''Woops, looks like you didn't provide a set of parameters 'algoParams' suitable for the clustering algorithm you intent to use, 'SpectralClustering'.
                        'algoParams' must be a dictionnary containing at least one of these items:
                        {'n_clusters':8, 'eigen_solver':None, 'random_state':None, 'n_init':10, 'gamma':1.0, 'affinity':'rbf', 'n_neighbors':10, 'eigen_tol':0.0, 'assign_labels':'kmeans', 'degree':3, 'coef0':1, 'kernel_params':None, 'n_jobs':1}''')

            elif algo=="Nearest-Neighbors" or algo=="Nearest-Neighbours" or algo=="Nearest-neighbors" or algo=="nearest-neighbors" or algo=="nearest-Neighbors": # Not tested
                try:
                    model = NearestNeighbors(n_neighbors=2, algorithm='ball_tree')
                except:
                    print('''Woops, looks like you didn't provide a set of parameters 'algoParams' suitable for the clustering algorithm you intent to use, 'Nearest-Neighbors'.
                        'algoParams' must be a dictionnary containing at least one of these items:
                        {'n_neighbors':5, 'radius':1.0, 'algorithm':'auto', 'leaf_size':30, 'metric':'minkowski', 'p':2, 'metric_params':None, 'n_jobs':1}''')
            else:
                print("You misdialed the name of the clustering algorithm you intent to use. 'K-means', 'SpectralClustering' or 'NearestNeighbors' are available.")

            # Fit your model to the unLabeled data
            model.fit(featuresMatrix)  

            # Extract the labels your model have given to the datapoints  
            clusteringLabels = model.predict(featuresMatrix)
            self.clusteringFeaturesDF['Labels']=pd.Series(clusteringLabels, index=self.clusteringFeaturesDF.index)
            self.data.extractedFeaturesDF['Labels']=pd.Series(clusteringLabels, index=self.data.extractedFeaturesDF.index)
            self.clusteringCentroids = model.cluster_centers_

            
            # return self.clusteringFeaturesDF
        
    
    def classifying(self, unitsList='all', featuresList=None, algo="K-nearest-neighbors", n_clust_range=[4,5,6,7,8,9,10,11,12,13],algoParams={'n_neighbors':5, 'weights':'uniform', 'algorithm':'auto', 'leaf_size':30, 'p':2, 'metric':'minkowski', 'metric_params':None, 'n_jobs':1,}, standardize=True, normalize=True, again=False):
        '''Supervised learning to sort according to cell type. Algorithm Single Vector Machine.'''

        try:
            print("Units already classified.\n", self.classifyingFeaturesDF)
            if again==True:
                if input(" -- Classify again? Dial <anything> for yes, <enter> for no: "):
                    raise

        except:
            ## Preprocess data if required
            if standardize==True or normalize==True:
                self.preProcessFeatures(standardize=standardize, normalize=normalize)


            ## Select features to use
            allfeaturesList = ["MFRf0", "CCGf0", "CCGf1", "CCGf2", "CCGf3", "CCGf4", "CCGf5", "ISIf0", "ISIf1", "ISIf2"]  
            if (featuresList == None or featuresList == []):
                featuresList = []
                
                while 1:
                    idx = input("\n\nPlease dial a feature to use for the clustering - MFRf0, CCGf0-5, ISIf0-2 or <all> for all features; dial <d> if you are done: ")
                    if idx == "all":
                        featuresList = allfeaturesList
                        break
                    elif idx == "d":
                        break
                    elif idx in allfeaturesList:
                        featuresList.append(idx)
                    else:
                        print("\nYou must dial <MFRf0>, <CCGf0>, <CCGf1>, <CCGf2>, <CCGf3>, <CCGf4>, <CCGf5>, <ISIf0>, <ISIf1> or <ISIf2>.")
            
                if featuresList==[]:
                    print("\nYou didn't provide any feature. All features will then be used.")
                    featuresList = allfeaturesList

            if unitsList=="all":
                allunits=True
                unitsList=self.data.usedUnits   

            print("\n\n--> Units to classify: ", "all" if allunits==True else unitsList, "\n\n--> Features used for classifying: ", featuresList, "\n\n")
        
            if standardize==True or normalize==True:
                featuresDataframe = pd.DataFrame(data=self.preProcessedFeatures, index=self.data.usedUnits, columns = allfeaturesList)
            else:
                featuresDataframe = pd.DataFrame(data=self.data.extractedFeatures, index=self.data.usedUnits, columns = allfeaturesList)
            self.classifyingFeaturesDF = featuresDataframe.filter(featuresList, axis=1)
            featuresMatrix = self.classifyingFeaturesDF.as_matrix()



            ## Classifying

            # import preLabeled data (labeled by eye (08/05/2017) or optogenetics). 1D numpy array or text file.
            #preLabData = np.load(preLabeledData.npy)
            #preLabData = np.loadtxt(preLabeledData.txt)
            
            # Pick a model
            if algo=="K-nearest-neighbors":
                model = KNeighborsClassifier(**algoParams)       
            if algo=="Support-Vector-Machine":
                model = svm.SVC(**algoParams)

            # Fit the model to the train preLabeled data + predict the test preLabeled data labels, to compute accuracy.
            # random_state: seeds the split, stratify: allows to take into account ditribution of features to make train and test data equivalent.
            preLabData_train, preLabData_test, labels_train, labels_test = train_test_split(preLabData, labels, test_size=0.3, random_state=21, stratify=labels)
            model.fit(preLabData_train, labels_train)
            accuracy = model.score(preLabData_test, labels_test)
            print("Accuracy of the classifier: {}".format(accuracy))
            # Fit the model to the whole preLabeled data.
            model.fit(preLabData)
            # Predict the labels of the unLabeled data.
            classifyingLabels = model.predict(featuresMatrix)
            self.classifyingFeaturesDF['Labels']=pd.Series(clusteringLabels, index=self.clusteringFeaturesDF.index)
            self.classifyingCentroids = model.cluster_centers_
            
            return self.classifyingFeaturesDF


    def inspect_labels(self, featuresList=None, unitsList="all", algo=["Clustering", "K-means"], standardize=True, normalize=True, again=False, plot_crossTab=False, plot_scatter_matrix=True, mpl_style='classic'):

        matplotlib.style.use(mpl_style)
        
        allfeaturesList = ["MFRf0", "CCGf0", "CCGf1", "CCGf2", "CCGf3", "CCGf4", "CCGf5", "ISIf0", "ISIf1", "ISIf2"]
        TIME = time.strftime("%Y.%m.%d-%H.%M")

        ## Choose 2 or 3 features to display (in 2D or 3D)
        while 1:
            if (featuresList == None or featuresList == []):
                featuresList = []          
                while 1:
                    if len(featuresList)==3:
                        print("Three features already provided, time to visualize labels.")
                        break
                    idx = input("\n\nPlease dial 2 or three features to use for the clusters visualization - MFRf0, CCGf0-5 or ISIf0-2; dial <d> if you are done: ")

                    if idx == "d":
                        if len(featuresList)<2:
                            print("You must at least provide 2 features. Only ", len(featuresList), " provided.")
                        else:
                            break
                    elif idx in allfeaturesList:
                        featuresList.append(idx)
                    else:
                        print("\nYou must dial <MFRf0>, <CCGf0>, <CCGf1>, <CCGf2>, <CCGf3>, <CCGf4>, <CCGf5>, <ISIf0>, <ISIf1> or <ISIf2>.")
            
                if featuresList==[]:
                    print("\nYou didn't provide any feature. You need to provide 2 or 3 features.")
                    break
            
            print("\n\n --> Features to plot: ", featuresList)


            ## Choose units to use
            if unitsList=="all":
                unitsList=self.data.usedUnits


            ## Which algo was used: clustering or classifying
            if algo[0]=="Clustering":     
                try:
                    data = self.clusteringFeaturesDF.drop('Labels', axis=1, inplace=False)
                    labels = self.clusteringFeaturesDF['Labels'].tolist()
                    centroids = self.clusteringCentroids
                except:
                    self.clustering(algo=algo[1], standardize=standardize, normalize=normalize, again=again)
                    data = self.clusteringFeaturesDF.drop('Labels', axis=1, inplace=False)
                    labels = self.clusteringFeaturesDF['Labels'].tolist()
                    centroids = self.clusteringCentroids
            elif algo[0]=="Classifying":
                try:
                    data = self.classifyingFeaturesDF.drop('Labels', axis=1, inplace=False)
                    labels = self.classifyingFeaturesDF['Labels'].tolist()
                    centroids = self.classifyingCentroids
                except:
                    self.classifying(algo=algo[1], standardize=standardize, normalize=normalize, again=again)
                    data = self.classifyingFeaturesDF.drop('Labels', axis=1, inplace=False)
                    labels = self.classifyingFeaturesDF['Labels'].tolist()
                    centroids = self.classifyingCentroids


            ## Cross-tabulation with expected labels (from eye first, then with ground truth data)
            # Add the labeled celltypes to the DataFrame
            if plot_crossTab==True:
                if algo[0]=="Clustering":
                    pushLoadFile=False
                    try:
                        print("Labels attributed by human: ", self.clusteringFeaturesDF['cellTypes'])
                        # If no error raised == human labels already attributed
                        try:
                            assert len(np.unique(self.clusteringFeaturesDF['cellTypes'])) == len(np.unique(self.clusteringFeaturesDF['Labels']))
                        except:
                            while 1:
                                dec = input("The number of human labels - {} - does not correspond to the number of automatically attributes clusters - {} -. Attribute labels again <a> or use an other humanLabels file <o> or quit <q>.".format(len(np.unique(self.clusteringFeaturesDF['cellTypes'])), len(np.unique(self.clusteringFeaturesDF['Labels']))))
                                if dec=='a':
                                    raise
                                elif dec=='o':
                                    pushLoadFile==True
                                    raise
                                elif dec=='q':
                                    break
                                else:
                                    print("Warning: please write either <a>, <o> or <q>.")
                    except:
                        # Force the user to give to proper number of human labels (has to fit the clustering/classifying number of clusters/classes to compare them)
                        self.attribute_humanLabels(n_humanLabels=len(np.unique(self.clusteringFeaturesDF['Labels'])), pushLoadFile=pushLoadFile, askAgain=False, clustering=False, crossTabProcess=True)

                elif algo[0]=="Classifying":
                    pushLoadFile=False
                    try:
                        print("Labels attributed by human: ", self.classifyingFeaturesDF['cellTypes'])
                        # If no error raised == human labels already attributed
                        try:
                            assert len(np.unique(self.classifyingFeaturesDF['cellTypes'])) == len(np.unique(self.classifyingFeaturesDF['Labels']))
                        except:
                            while 1:
                                dec = input("The number of human labels - {} - does not correspond to the number of automatically attributes classes - {} -.".format(len(np.unique(self.classifyingFeaturesDF['cellTypes'])), len(np.unique(self.classifyingFeaturesDF['Labels']))))
                                if dec=='a':
                                    raise
                                elif dec=='o':
                                    pushLoadFile=True
                                    raise
                                elif dec=='q':
                                    break
                                else:
                                    print("Warning: please write either <a>, <o> or <q>.")
                    except:
                        # Force the user to give to proper number of human labels (has to fit the clustering/classifying number of clusters/classes to compare them)
                        self.attribute_humanLabels(n_humanLabels=len(np.unique(self.classifyingFeaturesDF['Labels'])), pushLoadFile=pushLoadFile)
                    

                # Create crosstab: ct
                if algo[0]=="Clustering":
                    ct = pd.crosstab(self.clusteringFeaturesDF['Labels'], self.clusteringFeaturesDF['cellTypes'])
                elif algo[0]=="Classifying":
                    ct = pd.crosstab(self.classifyingFeaturesDF['Labels'], self.classifyingFeaturesDF['cellTypes'])

                # Save it as eps/png
                fig = plt.figure()
                ax = fig.add_subplot(111, frame_on=False)# no visible frame
                ax.xaxis.set_visible(False)  # hide the x axis
                ax.yaxis.set_visible(False)  # hide the y axis
                table(ax, ct)  # where df is your data frame
                if not os.path.exists(self.data.__dir__+'/XtraDataManager/ClusteringResults'):
                    os.makedirs(self.data.__dir__+'/XtraDataManager/ClusteringResults')
                figPath = self.data.__dir__+'/XtraDataManager/ClusteringResults/'+TIME+'_Cross-Tabulation'
                fig.savefig(figPath+'.eps')
                fig.savefig(figPath+'.png')


            ## Exploratory Data Analysis

            # Plot scatter matrix
            if plot_scatter_matrix==True:
                try:
                    scatterMatrix = pd.plotting.scatter_matrix(data.drop('cellTypes', axis=1), c=labels, figsize=[20,20], s=10, marker='o', alpha=0.5, hist_kwds={'color':['burlywood']})
                except:
                    scatterMatrix = pd.plotting.scatter_matrix(data, c=labels, figsize=[20,20], s=10, marker='o', alpha=0.5, hist_kwds={'color':['burlywood']})
                fig = scatterMatrix[0][0].get_figure()
                fig.suptitle("Scatter Matrix of clustering features", fontsize=14, fontweight='bold')
                if not os.path.exists(self.data.__dir__+'/XtraDataManager/ClusteringResults'):
                    os.makedirs(self.data.__dir__+'/XtraDataManager/ClusteringResults')
                figPath = self.data.__dir__+'/XtraDataManager/ClusteringResults/'+TIME+'_Scatter-Matrix'
                fig.savefig(figPath+'.eps')
                fig.savefig(figPath+'.png')

            # Plot 2 features
            if len(featuresList)==2:
                x = data[featuresList[0]].tolist()
                y = data[featuresList[1]].tolist()
                idx_x = data.columns.get_loc(featuresList[0])
                idx_y = data.columns.get_loc(featuresList[1])
                centroids_x = centroids[:,idx_x]
                centroids_y = centroids[:,idx_y]

                fig = plt.figure()
                ax = fig.add_subplot(111)
                ax.scatter(x, y, c=labels, alpha=0.5)
                ax.scatter(centroids_x, centroids_y, marker='D', color='r')
                ax.set_xlabel(str(featuresList[0]), fontsize=10)
                ax.set_ylabel(str(featuresList[1]), fontsize=10)
                ax.set_title("Clustering results - "+str(algo[1]), fontsize=14, fontweight='bold')

                if not os.path.exists(self.data.__dir__+'/XtraDataManager/ClusteringResults'):
                    os.makedirs(self.data.__dir__+'/XtraDataManager/ClusteringResults')
                figPath = self.data.__dir__+'/XtraDataManager/ClusteringResults/'+TIME+'_'+str(algo[1])+'-Feat: '+str(featuresList[0])+', '+str(featuresList[1])
                fig.savefig(figPath+'.eps')
                fig.savefig(figPath+'.png')
                plt.show()
                break

            # Plot 3 features
            elif len(featuresList)==3:
                x = data[featuresList[0]].tolist()
                y = data[featuresList[1]].tolist()
                z = data[featuresList[2]].tolist()
                idx_x = data.columns.get_loc(featuresList[0])
                idx_y = data.columns.get_loc(featuresList[1])
                idx_z = data.columns.get_loc(featuresList[2])
                centroids_x = centroids[:,idx_x]
                centroids_y = centroids[:,idx_y]
                centroids_z = centroids[:,idx_z]

                fig = plt.figure()
                ax = fig.add_subplot(111, projection='3d')
                ax.scatter(xs=x, ys=y, zs=z, c=labels, alpha=0.5)
                ax.scatter(xs=centroids_x, ys=centroids_y, zs=centroids_z, marker='D', color='r')
                ax.set_xlabel(str(featuresList[0]), fontsize=10)
                ax.set_ylabel(str(featuresList[1]), fontsize=10)
                ax.set_zlabel(str(featuresList[2]), fontsize=10)
                ax.set_title("Clustering results - "+str(algo[1]), fontsize=14, fontweight='bold')
                plt.show()
                break


'''



####### ####### ####### ####### ####### ####### ####### ####### ####### ####### #######
#######  SUPERVISED LEARNING METHOD: Classification, Support Vector Machine SVM  ######
####### ####### ####### ####### ####### ####### ####### ####### ####### ####### #######

classifier = svm.SVC(gamma=0.001)

# data has to be [nparray1, nparray2, ...] and target [targetIndex1, targetIndex2...]
# where nparray1... are arrays of features. Can they be array of arrays ?
classifier.fit(data[], target[])



# The digits dataset
digits = datasets.load_digits()

# The data that we are interested in is made of 8x8 images of digits, let's
# have a look at the first 4 images, stored in the `images` attribute of the
# dataset.  If we were working from image files, we could load them using
# matplotlib.pyplot.imread.  Note that each image must have the same size. For these
# images, we know which digit they represent: it is given in the 'target' of
# the dataset.
images_and_labels = list(zip(digits.images, digits.target))
for index, (image, label) in enumerate(images_and_labels[:4]):
    plt.subplot(2, 4, index + 1)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Training: %i' % label)

# To apply a classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

# Create a classifier: a support vector classifier
classifier = svm.SVC(gamma=0.001)

# We learn the digits on the first half of the digits
classifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])

# Now predict the value of the digit on the second half:
expected = digits.target[n_samples / 2:]
predicted = classifier.predict(data[n_samples / 2:])

print("Classification report for classifier %s:\n%s\n"
      % (classifier, metrics.classification_report(expected, predicted)))
print("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))

images_and_predictions = list(zip(digits.images[n_samples / 2:], predicted))
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
    plt.subplot(2, 4, index + 5)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Prediction: %i' % prediction)

plt.show()'''



'''Personal notes: Clustering with Scikit learn
    https://www.youtube.com/watch?v=ZueoXMgCd1c&index=34&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v
    
    -
    -
    -
    -
    
    '''

