# -*- coding utf-8 -*-
from __future__ import print_function

# This script gathers the routines to extract kilosort/phy (kwikteam) generated units features,
# stored in an instance of DataManager():
# - Sampling rate (30Khz)
# - Spike times in sample units
# - Spikes times in seconds
# - Spikes units, unit corresponding to each spike of Spike times
# - units indexes, array of the unit indexes (one occurence of each)
# - attributed Spike times and Spike samples, lists of n_units np arrays of the form [[unit_idx1, t1, t2...tn], ...] with t1... in seconds or samples
# - Instantaneous Firing rate, lists of n_units np arrays of the form [[unit_idx1, IFR1, IFR2...IFRn], ...]
# - CrossCorrelograms between units
# - Vizualisation tool

# >> How to use it: <<
# Go to the directory which contains the 1) params.py file, 2) spike_times.npy file, 3) spike_clusters.npy file, 4) spike_templates.npy file, 5) templates.npy file and 6) cluster_group.tsv file,
# all being generated by kilosort and exploited/modified by phy (kwikteam).
# Then launch python or ipython from any terminal, and "import XtraDataManager". Here it is, you can instanciate DataManager by doing "data = DataManager()".
# Then visualize the Mean Firing Rate, the Instantaneous Firing Rate and the auto/cross correlograms of units thanks to the visualization() method of this instanciation.

# Maxime Beau, 2017-05-10 


import numpy as np
from scipy import signal
import random
import time
from time import sleep
import progressbar

import scipy
import scipy.stats
import scipy.io
import pandas as pd
import matplotlib
matplotlib.use('TkAgg')
from matplotlib import pyplot as plt
from pylab import get_current_fig_manager
#matplotlib.style.use('fivethirtyeight')
#matplotlib.style.use('ggplot')
matplotlib.style.use('classic')
#matplotlib.style.use('dark_background')
import dill
import csv
import os, sys
#sys.path.append("/Users/maximebeau/phy")
import phy
from phy.utils._types import _as_array # phy, kwikteam
from phy.io.array import _index_of, _unique # phy, kwikteam

def smoothCCG(CCG, win=13, pol=2):
	import scipy.signal as signal
	return signal.savgol_filter(CCG, win, pol)

def plotCCGTEST(data, i, bin_sizeCCG=0.0002, window_sizeCCG=0.15, win=13, pol=2):
        import matplotlib.pyplot as plt
        import numpy as np
        idx = np.where(data.units==i)[0][0]
        CCG = data.CCG[idx][idx]
        plt.plot(CCG)
        plt.plot(smoothCCG(CCG, win=win, pol=pol))
        idx1 = np.where(data.usedUnits==i)[0][0]
        x1 = (1./data.extractedFeatures[idx1][1])/bin_sizeCCG
        print ("peakIndexFromCenter: ", x1)
        halfCCG = (len(CCG)-1)/2
        x1+=halfCCG
        x2 = x1
        y1, y2 = 0, CCG.max()
        plt.plot((x1,x2),(y1,y2),'-k')
        plt.show()
        plt.close()

def indices(a, func):
    return [i for (i, val) in enumerate(a) if func(val)]

def _increment(arr, indices):
    """Increment some indices in a 1D vector of non-negative integers.
    Repeated indices are taken into account."""
    arr = _as_array(arr)
    indices = _as_array(indices)
    bbins = np.bincount(indices)
    arr[:len(bbins)] += bbins
    return arr

def _diff_shifted(arr, steps=1):
    arr = _as_array(arr)
    return arr[steps:] - arr[:len(arr) - steps]

def _create_correlograms_array(n_units, winsize_bins):
	'''3D matrix of n_units x n_units x window size in amount of bins'''
	return np.zeros((n_units, n_units, winsize_bins // 2 + 1), dtype=np.int32)

def _symmetrize_correlograms(correlograms):
    """Return the symmetrized version of the CCG arrays."""

    n_units, _, n_bins = correlograms.shape
    assert n_units == _

    # We symmetrize c[i, j, 0].
    # This is necessary because the algorithm in correlograms()
    # is sensitive to the order of identical spikes.
    correlograms[..., 0] = np.maximum(correlograms[..., 0],
                                      correlograms[..., 0].T)

    sym = correlograms[..., 1:][..., ::-1]
    sym = np.transpose(sym, (1, 0, 2))

    return np.dstack((sym, correlograms))

def load_DM(filename):
	'''Arguments: "data"=NameOfDataManager, "filename" has to be of the form "xxxxxxx.pkl".'''
	with open(filename, 'rb') as Input:
		return dill.load(Input)

class DataManager():
	'''Has to be informed of the directory of kilosort output (files reshaped by phy).
		The directory can be provided as argument. If no argument is provided, the used directory is the current working directory.

		methods() -> respective generated attributes:

		chdir() -> .__dir__: directory to seek for data and generate plots.

		load_sampleRate() -> .sample_rate: Sampling rate (30Khz)

		load_spsamples() -> .spike_samples: Spike times in sample units

		calcul_sptimes() -> .spike_times: Spikes times in seconds

		load_spunits() -> .spike_clusters: Spikes units, unit corresponding to each spike of Spike times

		extract_cIds() -> .units, .spike_clusters_i: units numbers (unique occurence) and indexes in the array

		attribute_spikeSamples_Times() -> .attributed_spikeSamples, .attributed_spikeSamplesDic, .attributed_spikeTimes, .attributed_spikeTimesDic: 
			attributed Spike times and Spike samples,
			lists of n_units np arrays of the form [np.array([unit_idx1, t1, t2...tn]), ...]
			or dictionnaries of the form {unitIndex: np.array([t1, t2...tn]), ...}
			with t1... in seconds or samples.

		attribute_spikeTemplates() -> .attributed_spikeTemplatesIdx, self.attributed_spikeTemplates, self.attributed_spikeTemplatesDic: 
			attributed Spike templates, 
			lists of n_units np arrays of the form [np.array([unit_idx1, v1, v2, v3... vn_timepoints]), ...]
			or dictionnaries of the form {unitIndex: np.array([v1, v2... vn_timepoints]), ...}
			with v1... being the units templates points (np arrays storing n_timepoints values).

		InstFR() -> .IFR: Instantaneous Firing rate, lists of n_units np arrays of the form [[unit_idx1, IFR1, IFR2...IFRn], ...]

		MeanFR() -> .MFR: Mean Firing rate, lists of n_units np arrays of the form [[unit_idx1, MFR], ...]

		CCG() -> .correlograms: all crossCorrelograms in a n_units x n_units x winsize_bins matrix.

		visualize() -> Visualization tool of DataManager attributes.
			Argument1: list of units whose features need to be visualized (int or float). [unit1, unit2...]
			Argument2: list of features to visualize (str). Can contain "IFR": Instantaneous Firing Rate, "MFR": Mean Firing Rate, "CCG": CrossCorreloGrams, "WVF": weighted averaged templates.

		save() -> saving a DataManager() instance. Argument: filename, has to be of the form "xxxxxxx.pkl".'''

	def __init__(self, directory=None):
		if directory==None:
			directory = os.getcwd()
		elif directory==1:
			directory='/Volumes/DK_students1/2017-04-08'
		elif directory==2:
			directory='/Users/maximebeau/Desktop/Science/5_Master_2/Internship/Data_Analysis/debugCTC'
		os.chdir(directory)
		self.__dir__ = directory
		if not os.path.exists(self.__dir__+'/XtraDataManager/'):
			os.makedirs(self.__dir__+'/XtraDataManager/')

	def chdir(self, directory="/Volumes/DK_students1/2017-04-08"):
		'''chdir() -> .__dir__: directory to seek for data and generate plots.'''
		os.chdir(directory)
		self.__dir__ = directory

	def load_sampleRate(self, again=False):
		'''load_sampleRate() -> .sample_rate: Sampling rate (30Khz)'''
		try:
			print("Sample rate already loaded: ", self.sample_rate)
			if again==True:
				if input(" -- Load again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
			import params
			self.sample_rate = params.sample_rate
			print("Sample rate loaded.")
		return self.sample_rate

	def load_spsamples(self, again=False):
		'''load_spsamples() -> .spike_samples: Spike times in sample units'''
		try:
			print("Spike samples already loaded. Array shape:", self.spike_samples.shape)
			if again==True:
				if input(" -- Load again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
			self.spike_samples = np.load("spike_times.npy")
			self.spike_samples = self.spike_samples.flatten()
			self.spike_samples = np.asarray(self.spike_samples, dtype=np.int64)
			print("Spike samples loaded.")
		return self.spike_samples

	def calcul_sptimes(self, again=False):
		'''calcul_sptimes() -> .spike_times: Spikes times in seconds'''
		try:
			print("Spike times already calculated. Array shape:", self.spike_times.shape)
			if again==True:
				if input(" -- Calcul again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
			self.load_sampleRate(again=again)
			self.load_spsamples(again=again)
			self.spike_times = self.spike_samples/self.sample_rate
			self.spike_times = np.asarray(self.spike_times, dtype=np.float64)
			print("Spike times calculated.")
		return self.spike_times

	def load_spunits(self, again=False):
		'''load_spunits() -> .spike_clusters: Spikes units, corresponding to each spike of Spike times'''
		try:
			print("Spike units already loaded. Array shape:", self.spike_clusters.shape)
			if again==True:
				if input(" -- Load again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
			self.spike_clusters = np.load("spike_clusters.npy")
			print("Spike units loaded.")
		return self.spike_clusters

	def extract_cIds(self, again=False):
		'''extract_cIds() -> .units, .spike_clusters_i: units numbers (unique occurence) and indexes in the array'''
		try:
			print("unit Ids already extracted. Array shape:", self.units.shape)
			if again==True:
				if input(" -- Extract again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
			self.load_spunits(again=again)
			self.units = _unique(self.spike_clusters)
			self.spike_clusters_i = _index_of(self.spike_clusters, self.units)
			self.units = np.asarray(self.units, dtype=np.int64)

			self.goodUnits = np.array([])
			with open("cluster_group.tsv", "r") as readingTSV: 
				readingTSV = csv.DictReader(readingTSV, delimiter="\t")
				for row in readingTSV:
					if row["group"]=="good":
						self.goodUnits = np.append(self.goodUnits, np.array([int(row["cluster_id"])]))
			#self.spike_clusters_i = np.asarray(self.spike_clusters_i, dtype=np.float64)
			print("units Ids extracted.")
		return self.units, self.goodUnits, self.spike_clusters_i

	def load_sptemplates(self, again=False):
		'''load_sptemplates() -> .spike_templates: Spike templates, corresponding to each spike of Spike times'''
		try:
			print("Spike templates already loaded. Array shape:", self.spike_templates.shape)
			if again==True:
				if input(" -- Load again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
			self.spike_templates = np.load("spike_templates.npy")
			self.spike_templates = self.spike_templates.flatten()
			self.spike_templates = np.asarray(self.spike_templates, dtype=np.int64)
			print("Spike templates loaded.")
			self.templates = np.load("templates.npy")
			self.templates = np.asarray(self.templates, dtype=np.float64)
			print("Templates loaded.")
		return self.spike_templates, self.templates

	def attribute_spikeSamples_Times(self, again=False):
		'''attribute_spikeSamples_Times() -> .attributed_spikeSamples, .attributed_spikeSamplesDic, .attributed_spikeTimes, .attributed_spikeTimesDic:
		attributed Spike times and Spike samples, lists of n_units np arrays of the form 
		[[unit_idx1, t1, t2...tn], ...] 
		with t1... in seconds or samples'''
		try:
			print("Spike samples and times already attributed. List length:", len(self.attributed_spikeTimes))
			if again==True:
				if input("Attribute again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
			self.calcul_sptimes(again=again)
			self.extract_cIds(again=again)
			self.attributed_spikeSamples = []
			self.attributed_spikeSamplesDic = {}
			self.attributed_spikeTimes = []
			self.attributed_spikeTimesDic = {}
			for UNIT in self.units:
				if UNIT in self.goodUnits:
					d = np.where(self.spike_clusters==UNIT)
					unit = np.array([UNIT], dtype=np.int64)
					arr1 = np.append(unit, self.spike_samples[d])
					self.attributed_spikeSamples.append(arr1)
					self.attributed_spikeSamplesDic[UNIT]=self.spike_samples[d]
					arr2 = np.append(unit, self.spike_samples[d]*1./self.sample_rate)
					self.attributed_spikeTimes.append(arr2)
					self.attributed_spikeTimesDic[UNIT]=self.spike_samples[d]*1./self.sample_rate

			#self.attributed_spikeTimes = [x.copy() for x in self.attributed_spikeSamples]
			#self.attributed_spikeTimesDic = self.attributed_spikeSamplesDic.copy()
			#for i, x in enumerate(self.attributed_spikeTimes):
			#	self.attributed_spikeTimes[i][1:] = x[1:]/self.sample_rate
			#for key, val in self.attributed_spikeTimesDic.items():
			#	self.attributed_spikeTimesDic[key]=val/self.sample_rate

			print("Spike samples and times attributed.")
		return self.attributed_spikeSamples, self.attributed_spikeSamplesDic, self.attributed_spikeTimes, self.attributed_spikeTimesDic

	def attribute_spikeTemplates(self, again=False):
		'''attribute_spikeTemplates() -> .attributed_spikeTemplatesIdx, self.attributed_spikeTemplates, self.attributed_spikeTemplatesDic: 
		attributed Spike templates, lists of n_units np arrays of the form 
		[[unit_idx1, t1, t2, t1, t3...tn], ...] 
		with t1... being the templates (np arrays storing n_timepoints values)'''
		try:
			print("Spike templates attributed. List length:", len(self.attributed_spikeTemplates))
			if again==True:
				if input(" -- Attribute again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
			self.load_sptemplates(again=again)
			self.extract_cIds(again=again)
			# Attribute template indexes and counts to each unit
			self.attributed_spikeTemplatesIdx = {}
			for UNIT in self.units:
				if UNIT in self.goodUnits:
					d = np.where(self.spike_clusters==UNIT)
					unique, counts = np.unique(self.spike_templates[d], return_counts=True)
					self.attributed_spikeTemplatesIdx[UNIT]=dict(zip(unique, counts))
			# attribute the templates themselves (weighted average) on the max amplitude channel
			self.attributed_spikeTemplates = []
			self.attributed_spikeTemplatesDic = {}
			for UNIT, UNITTemps in self.attributed_spikeTemplatesIdx.items():
				template = np.zeros(len(self.templates[0]))
				totalCounts = 0
				for tempIdx, tempCounts in UNITTemps.items():
					channelAbsMax = np.where(abs(self.templates[tempIdx-1]) == abs(self.templates[tempIdx-1]).max())[1][0] # Find the channel where the template has the maximum absolute amplitude
					template += np.array([self.templates[tempIdx-1][i][channelAbsMax] for i in range(len(self.templates[tempIdx-1]))], dtype=np.float64)*tempCounts # self.templates[0] accounts for template #1
					totalCounts+=tempCounts
				template/=totalCounts
				unit = np.array([UNIT], dtype=np.int64)
				arr = np.append(unit, template)
				self.attributed_spikeTemplates.append(arr)
				self.attributed_spikeTemplatesDic[UNIT]=template


			print("Spikes templates attributed.")
		return self.attributed_spikeTemplatesIdx, self.attributed_spikeTemplates, self.attributed_spikeTemplatesDic

	def InstFR(self, binsize = 0.003, sd = 10, again=False, convolve=True):
		'''InstFR() -> .IFR: Instantaneous Firing rate, lists of n_units np arrays of the form [[unit_idx1, IFR1, IFR2...IFRn], ...].
		Binsize is in seconds.'''
		try:
			self.IFR = np.load(self.__dir__+"/XtraDataManager/IFRarray_bin{}_sd{}.npy".format(binsize, sd))
		except: # No CCG file has ever been saved with these window and bin values
			pass

		try:
			print("IFR already calculated. List length:", len(self.IFR))
			if again==True:
				if input(" -- Calcul again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
			self.attribute_spikeSamples_Times(again=again)
			gaussian = signal.gaussian(90, sd)
			self.IFRhist = self.attributed_spikeSamples.copy()
			self.IFRconv = self.attributed_spikeSamples.copy()
			self.IFRhistDic = {}
			self.IFRconvDic = {}
			binsize*=self.sample_rate
			binEdges = np.arange(0, self.spike_samples[-1], binsize)

			bar = progressbar.ProgressBar(maxval=len(self.attributed_spikeSamples), \
			widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])
			print("\nInstantaneous Firing rates calcul in progress...")
			bar.start()

			for i, x in enumerate(self.attributed_spikeSamples):
				bar.update(i+1)
				sleep(0.01)
				UNIT = x[0]
				if UNIT in self.goodUnits:
					hist = np.histogram(x[1:], binEdges)
					conv = np.convolve(hist[0], gaussian)
					self.IFRhist[i] = np.append(UNIT, hist[0])
					self.IFRconv[i] = np.append(UNIT, conv)
					self.IFRhistDic[UNIT]=hist[0]
					self.IFRconvDic[UNIT]=conv
			bar.finish()
			print("Instantaneous Firing rates calculated.\n")
			self.IFRhist = np.asarray(self.IFRhist)
			self.IFRconv = np.asarray(self.IFRconv)

			if convolve==True:
				self.IFR = self.IFRconv
			else:
				self.IFR=self.IFRhist

			np.save(self.__dir__+"/XtraDataManager/IFRarray_bin{}_sd{}.npy".format(binsize, sd), self.IFR)
				
		return self.IFR

	def MeanFR(self, again=False):
		'''MeanFR() -> .MFR: Mean Firing rate, lists of n_units np arrays of the form [np.array([unit_idx1, MFR]), ...]'''
		self.attribute_spikeSamples_Times(again=again)
		# try:
		# 	self.MFR = np.load(self.__dir__+"/XtraDataManager/MFRarray.npy")
		# 	self.MFRDic = {self.goodUnits[i]:self.MFR[i] for i in range(len(self.MFR))}
		# except: # No CCG file has ever been saved with these window and bin values
		# 	pass

		try:
			print("MFR already calculated. List length:", len(self.MFR))
			if again==True:
				if input(" -- Calcul again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
            #Approximate length of the whole recording, in seconds
			recordLen = float(self.spike_times[-1])           
			# Calculate MFR
			self.MFR = []
			self.MFRDic = {}

			bar = progressbar.ProgressBar(maxval=len(self.attributed_spikeTimes), \
			widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])
			print("\nMean firing rates calcul in progress...")
			bar.start()

			for i, x in enumerate(self.attributed_spikeTimes):
				bar.update(i+1)
				sleep(0.01)
				UNIT = x[0]
				if x[0] in self.goodUnits:
					# Remove 1min shanks without any spikes
					shanksSize = 1 # seconds
					recordLenWithSpikes = 0
					for t in range(int(recordLen/shanksSize)):
						shank = (t*shanksSize, (t+1)*shanksSize)
						spikesInShank = np.where(np.logical_and(self.attributed_spikeTimes[i][1:]>=shank[0], self.attributed_spikeTimes[i][1:]<=shank[1]))
						if spikesInShank[0].any():
							recordLenWithSpikes+=shanksSize
						if recordLenWithSpikes == int(recordLen/shanksSize)*shanksSize:
							recordLenWithSpikes = recordLen
					self.MFR.append(np.array([self.attributed_spikeTimes[i][0], float(len(self.attributed_spikeTimes[i][1:]))/recordLenWithSpikes]))
					self.MFRDic[self.attributed_spikeTimes[i][0]] = float(len(self.attributed_spikeTimes[i][1:]))/recordLenWithSpikes
			bar.finish()
			print("Mean firing rates calculated.\n")
			self.MFR = np.asarray(self.MFR)
			np.save(self.__dir__+"/XtraDataManager/MFRarray.npy", self.MFR)
		return self.MFR, self.MFRDic

	def CrossCG(self, bin_size=0.0002, window_size=0.1, symmetrize=True, normalize = True, again=False):
		'''CCG() -> .CCG: all crossCorrelograms in a n_units x n_units x winsize_bins matrix.By default, bin_size=0.0002 and window_size=0.080, in seconds.'''
		self.calcul_sptimes(again=again)
		self.extract_cIds(again=again)
		try:
			self.CCG = np.load(self.__dir__+"/XtraDataManager/CCGarray_win{}_bin{}.npy".format(window_size, bin_size))
		except: # No CCG file has ever been saved with these window and bin values
			pass

		try:
			print("CrossCorrelograms already computed.", len(self.CCG)) # If it has already been calculated in this session OR loaded two lines above
			if again==True:
				if input(" -- Compute again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
			assert self.sample_rate > 0.
			assert np.all(np.diff(self.spike_times) >= 0), ("The spike times must be increasing.")
			assert self.spike_samples.ndim == 1
			assert self.spike_samples.shape == self.spike_clusters.shape
			bin_size = np.clip(bin_size, 1e-5, 1e5)  # in seconds
			binsize = int(self.sample_rate * bin_size)  # in samples
			assert binsize >= 1

			window_size = np.clip(window_size, 1e-5, 1e5)  # in seconds
			winsize_bins = 2 * int(.5 * window_size / bin_size) + 1
			assert winsize_bins >= 1
			assert winsize_bins % 2 == 1


			n_units = len(self.units)

			# Shift between the two copies of the spike trains.
			shift = 1

		    # At a given shift, the mask precises which spikes have matching spikes
			# within the correlogram time window.
			mask = np.ones_like(self.spike_samples, dtype=np.bool)

			self.correlograms = _create_correlograms_array(n_units, winsize_bins)
			#print(" - CCG bins: ", winsize_bins)

			# The loop continues as long as there is at least one spike with
			# a matching spike.
			print("\nCrossCorrelograms calcul in progress...\n[Iteration", end = " ")
			bar = progressbar.ProgressBar(max_value=progressbar.UnknownLength)
			bar.start()
			i=0
			while mask[:-shift].any():
				bar.update(i+1)
				i+=1

		        # Number of time samples between spike i and spike i+shift.
				spike_diff = _diff_shifted(self.spike_samples, shift)

		        # Binarize the delays between spike i and spike i+shift.
				spike_diff_b = spike_diff // binsize

		        # Spikes with no matching spikes are masked.
				mask[:-shift][spike_diff_b > (winsize_bins // 2)] = False

		        # Cache the masked spike delays.
				m = mask[:-shift].copy()
				d = spike_diff_b[m]

		        # # Update the masks given the units to update.
		        # m0 = np.in1d(spike_clusters[:-shift], units)
		        # m = m & m0
		        # d = spike_diff_b[m]
				d = spike_diff_b[m]

		        # Find the indices in the raveled correlograms array that need
		        # to be incremented, taking into account the spike units.
				indices = np.ravel_multi_index((self.spike_clusters_i[:-shift][m], self.spike_clusters_i[+shift:][m], d), self.correlograms.shape)

		        # Increment the matching spikes in the correlograms array.
				_increment(self.correlograms.ravel(), indices)

				shift += 1
			bar.finish()
			# Remove ACG peaks
			self.correlograms[np.arange(n_units),
	                 np.arange(n_units),
	                 0] = 0

			if symmetrize==True:
				self.correlograms = _symmetrize_correlograms(self.correlograms)
			print("CrossCorrelograms computed.\n")
			
			if normalize==True:
				self.correlograms = np.apply_along_axis(lambda x: x/np.sum(x) if np.sum(x)!=0 else x, 2, self.correlograms)


			self.CCG = self.correlograms
			np.save(self.__dir__+"/XtraDataManager/CCGarray_win{}_bin{}.npy".format(window_size, bin_size), self.CCG)

		return self.CCG

	def InterSI(self, bin_size=0.0005, window_size=0.2, normalize = True, again=False):
		'''InterSI() -> .ISI: all interspike interval histograms in a list of the form [np.array([unit_idx1, ISIcounts1, ISIcounts2...]), ...]. By default, bin_size=0.0005 in seconds.'''
		self.attribute_spikeSamples_Times(again=again)
	
		try:
			print("InterSpikeIntervals already computed.", len(self.ISI))
			if again==True:
				if input(" -- Compute again? Dial <anything> for yes, <enter> for no: "):
					raise
		except:
			self.ISI = []
			self.ISIDic = {}
			self.ISIparams = []
			self.ISIparamsDic = {}
			self.ISIList = []
			self.ISIListDic = {}
			binEdges = np.arange(0, window_size+bin_size, bin_size) # in seconds
			alpha = getattr(scipy.stats, 'alpha')

			bar = progressbar.ProgressBar(maxval=len(self.attributed_spikeTimes), \
			widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])
			bar.start()
			print("\nInterSpikeIntervals calcul in progress...")

			for i, x in enumerate(self.attributed_spikeTimes):
				bar.update(i+1)
				sleep(0.01)
				ISIlist = list(np.diff(self.attributed_spikeTimes[i][1:]))
				ISIhist = np.histogram(ISIlist, binEdges)
				ISIparams = np.array([np.mean(ISIlist), np.var(ISIlist), scipy.stats.skew(ISIlist)])
				self.ISI.append(np.append(np.array([x[0]]), ISIhist[0]))
				self.ISIDic[x[0]]=ISIhist[0]
				self.ISIparams.append(np.append(np.array([x[0]]), ISIparams))
				self.ISIparamsDic[x[0]]=ISIparams
				self.ISIList.append(np.append(np.array([x[0]]), ISIlist))
				self.ISIListDic[x[0]]=ISIlist
			if normalize==True:
				self.ISI = np.apply_along_axis(lambda x: x/np.sum(x) if np.sum(x)!=0 else x, 1, self.ISI)

			bar.finish()
			print("InterSpikeIntervals computed.\n")
			np.save(self.__dir__+"/XtraDataManager/ISIarray_bin{}_win{}.npy".format(bin_size, window_size), self.ISI)

		return self.ISI, self.ISIDic

	def extractFeatures(self, featuresList=['MFR','CCG', 'ISI', 'WVF'],  bin_sizeCCG=0.0002, window_sizeCCG=0.1, bin_sizeISI=0.0005, again=False):
		'''extractFeatures() -> .extractedFeatures, creates a numy array of shape (n_units, n_features) compatible with scikit-learn uniting/classifying.
		.extractedFeatures[:,0]: MFR
		.extractedFeatures[:,1]: 1/CCG peak time
		.extractedFeatures[:,2]: Area under the curve of the 1st fifth of autocorrelogram/total area under the curve of autocorrelogram
		.extractedFeatures[:,3]: Area under the curve of the 2nd fifth of autocorrelogram/total area under the curve of autocorrelogram
		.extractedFeatures[:,4]: Area under the curve of the 3rd fifth of autocorrelogram/total area under the curve of autocorrelogram
		.extractedFeatures[:,5]: Area under the curve of the 4th fifth of autocorrelogram/total area under the curve of autocorrelogram
		.extractedFeatures[:,6]: Area under the curve of the 5th fifth of autocorrelogram/total area under the curve of autocorrelogram
		.extractedFeatures[:,7]: Mean of interspike intervals distribution
		.extractedFeatures[:,8]: Variance of interspike intervals distribution
		.extractedFeatures[:,9]: Skewness of interspike intervals distribution'''

		features = []

		self.nanUnits = np.array([])

		if 'MFR' in featuresList:
			self.MeanFR(again=again)
			self.MFRf0 = {}
			for unitIdx, unit in enumerate(self.units):
				if unit in self.goodUnits:
					self.MFRf0[unit]=self.MFRDic[unit]
			features.append(self.MFRf0)


		if 'CCG' in featuresList:
			self.CrossCG(bin_size=bin_sizeCCG, window_size=window_sizeCCG, again=again)

			self.CCGf0, self.CCGf1, self.CCGf2, self.CCGf3, self.CCGf4, self.CCGf5 = {}, {}, {}, {}, {}, {}
			for unitIdx, unit in enumerate(self.units):
				if unit in self.goodUnits:
					CCG = self.CCG[unitIdx][unitIdx]
					halfCCG = int((len(CCG)-1)/2)
					smoothCCG = signal.savgol_filter(CCG, 13, 2)
					diffCCG = np.diff(smoothCCG)/bin_sizeCCG

					if abs(np.where(CCG==CCG.max())[0][1]-halfCCG)<=(window_sizeCCG/bin_sizeCCG)*0.95: # If the maximum is not on the extremity = problem
						for i in range(halfCCG-1): # i max = halfCCG-2
							y1 = diffCCG[halfCCG+i] # Needs to start at diffCCG[halfCCG] = diff between CCG[halfCCG] (center) and CCG[halfCCG+1] (right after center)
							y2 = diffCCG[halfCCG+i+1] # Needs to end at diffCCG[halfCCG+halfCCG-1]
							#print("Unit:", unit," - y1:", y1, " - y2:", y2, " - i:", i)
							if y1>0 and y2<0:
								# peak ZONE found thanks to the smoothed ACG
								#print("PEAK ZONE FOUND: bin half+", i+1)
								peakWinHalf = int(1+0.01/bin_sizeCCG) # 10 for 1ms bins, 1 above 10ms size bins
								peakWindow = np.array([CCG[halfCCG+i+1+j] for j in np.arange(-peakWinHalf,peakWinHalf+1,1)])
								# peak INDEX found thanks to the not smoothed ACG, more accurate
								peakIndexFromLeft = (halfCCG+i+1 #Index of smoothed ACG peak
														+ np.where(peakWindow==peakWindow.max())[0][0]-peakWinHalf) # Index of unsmoothed ACG peak
								peakIndexFromCenter = abs(peakIndexFromLeft - halfCCG)
								self.CCGf0[unit] = 1./(peakIndexFromCenter*bin_sizeCCG)
								#print("peakIndexFromCenter:", peakIndexFromCenter)
								break
					else:
						self.nanUnits = np.append(self.nanUnits, np.array([unit]))
						self.CCGf0[unit] = np.nan

					if CCG[halfCCG:].sum()!=0:
						self.CCGf1[unit] = np.divide(CCG[halfCCG:int(halfCCG+halfCCG*1./5)].sum(), CCG[halfCCG:].sum()) # % of the area under the curve of the half CCG contained in the first 1/5th of the half CCG
						self.CCGf2[unit] = np.divide(CCG[int(halfCCG+halfCCG*1./5):int(halfCCG+halfCCG*2./5)].sum(), CCG[halfCCG:].sum()) # % of the area under the curve of the half CCG contained in the second 1/5th of the half CCG
						self.CCGf3[unit] = np.divide(CCG[int(halfCCG+halfCCG*2./5):int(halfCCG+halfCCG*3./5)].sum(), CCG[halfCCG:].sum()) # % of the area under the curve of the half CCG contained in the third 1/5th of the half CCG
						self.CCGf4[unit] = np.divide(CCG[int(halfCCG+halfCCG*3./5):int(halfCCG+halfCCG*4./5)].sum(), CCG[halfCCG:].sum()) # % of the area under the curve of the half CCG contained in the fourth 1/5th of the half CCG
						self.CCGf5[unit] = np.divide(CCG[int(halfCCG+halfCCG*4./5):].sum(), CCG[halfCCG:].sum()) # % of the area under the curve of the half CCG contained in the last 1/5th of the half CCG
					else:
						self.nanUnits = np.append(self.nanUnits, np.array([unit]))
						self.CCGf1[unit] = np.nan
						self.CCGf2[unit] = np.nan
						self.CCGf3[unit] = np.nan
						self.CCGf4[unit] = np.nan
						self.CCGf5[unit] = np.nan

			features.append(self.CCGf0)
			features.append(self.CCGf1)
			features.append(self.CCGf2)
			features.append(self.CCGf3)
			features.append(self.CCGf4)
			features.append(self.CCGf5)
		

		if "ISI" in featuresList:
			self.InterSI(bin_size=bin_sizeISI, again=again)
			self.ISIf0, self.ISIf1, self.ISIf2, self.CCGf3, self.CCGf4, self.CCGf5 = {}, {}, {}, {}, {}, {}
			for unitIdx, unit in enumerate(self.units):
				if unit in self.goodUnits:
					self.ISIf0[unit] = 1./self.ISIparamsDic[unit][0] # Mean of the ISI distribution (moment 1) ~ 1/mean firing rate
					self.ISIf1[unit] = self.ISIparamsDic[unit][1] # Variance of the ISI ditribution (moment 2)
					self.ISIf2[unit] = self.ISIparamsDic[unit][2] # Skewness of the ISI distribution (moment 3)
			features.append(self.ISIf0)
			features.append(self.ISIf1)
			features.append(self.ISIf2)

		# if 'WVF' in featuresList:
			# self.attribute_spikeTemplates()
			# for unitIdx, unit in enumerate(self.units):

		self.nanUnits = np.unique(self.nanUnits)
		self.usedUnits = np.array([i for i in self.goodUnits if i not in self.nanUnits])
		n_units = len(self.usedUnits)
		n_features = len(features)
		self.extractedFeatures = np.zeros((n_units, n_features))
		for unitIdx, unit in enumerate(self.usedUnits):
			self.extractedFeatures[unitIdx] += np.array([feat[unit] for feat in features])

		self.extractedFeaturesDF = pd.DataFrame(data=self.extractedFeatures, index=self.usedUnits, columns=["MFRf0", "CCGf0", "CCGf1", "CCGf2", "CCGf3", "CCGf4", "CCGf5", "ISIf0", "ISIf1", "ISIf2"])
		return self.extractedFeatures

	def visualize(self, unitsList=None, featuresList=None, showMode=None, saveMode=True, bin_sizeIFR=0.003, bin_sizeCCG=0.0002, window_sizeCCG=0.1, bin_sizeISI=0.0005, window_sizeISI=0.2, plotType='bar', normalizeCCG = True, again=False, mpl_style='classic'):
		'''visualize() -> Visualization tool.
		Argument1: list of units whose features need to be visualized (int or float). [unit1, unit2...]
		Argument2: list of features to visualize (str). Can contain "IFR": Instantaneous Firing Rate, "MFR": Mean Firing Rate, "CCG": CrossCorreloGrams, "ISI" InterSpikeInterval, "WVF": weighted averaged templates.'''
		matplotlib.style.use(mpl_style)

		if type(featuresList)==int or type(featuresList)==float:
			featuresList=list(featuresList)

		EXIT = False

		while 1:
			self.extract_cIds(again=again)

			if unitsList!=None and unitsList!=[]:
				for i in unitsList:
					if i not in self.goodUnits:
						print("Some units provided in unitsList are not good units. For performance reasons, only good units are managed. Resetting unitsList - provide other units in the coming lines please.")
						unitsList=[]

			elif unitsList == None or unitsList == []:
				unitsList = []
                
				print("Units classified as good: ", self.goodUnits.tolist())

				while 1:
					idx = input("\n\nCurrent units to visualize: {}\nPlease dial a unit index ; dial <d> if you are done: ".format(unitsList))

					if idx == "d":
						break
					else:
						try:
							idx = int(idx) # input() only returns strings
							if idx in self.units:
								if idx in self.goodUnits:
									print("\nThis unit is classified as good.")
									unitsList.append(idx)
								if idx not in self.goodUnits:
									print("\nThis unit is NOT classified as good.")
							else:
								print("\nThis index is not detected in the unit indexes of this directory's data. Try another one.")
						except:
							print("\nYou must dial a floatting point number or an integer.")

				if unitsList==[]:
					print("\nYou didn't provide any unit.")
					EXIT = True

			if featuresList=='all':
				featuresList = ['MFR', 'IFR', 'CCG', 'ISI', 'WVF']
			elif (featuresList == None or featuresList == []) and EXIT == False:
				featuresList = []

				while 1:
					idx = input("\n\nPlease dial a feature to visualize - <MFR>, <IFR>, <CCG>, <ISI>, <WVF> or <all>; dial <d> if you are done: ")
					
					if idx == "d":
						break
					elif idx == 'all':
						featuresList = ['MFR', 'IFR', 'CCG', 'ISI', 'WVF']
						break
					elif idx == 'MFR' or idx == 'IFR' or idx == 'CCG' or idx == 'ISI' or idx == 'WVF':
						featuresList.append(idx)
					else:
						print("\nYou must dial <MFR>, <IFR>, <CCG>, <ISI> or <WVF>.")

				if featuresList==[]:
					print("\nYou didn't provide any feature.")
					EXIT = True

			if EXIT==True:
				if showMode=='all':
					print("Features visualized.")
					plt.show()
					plt.close()
				print("\nYou used XtraDataManager's visualization tool.\n")
				break


			print("\n\n--> Units to visualize: ", unitsList, "\n\n--> Features displayed: ", featuresList, "\n\n")

			if "MFR" in featuresList:
				self.MeanFR(again=again)
				MFRList = []
				for i in unitsList:
					MFRidx = np.where(self.goodUnits==i)[0][0]
					MFRList.append(self.MFR[MFRidx][1])
				unitsListStr = [str(i) for i in unitsList]
				assert len(unitsListStr) == len(MFRList)

				dfMFR = pd.DataFrame(data=MFRList, index=unitsListStr, columns=["Mean Firing rate (Hz)"])
				axMFR = dfMFR.plot(kind='bar', rot=0, legend=False, width=0.1, linewidth=2, color=(51./255, 153./255, 255./255), edgecolor='k', hatch="-")
				axMFR.tick_params(labelsize=6, color='k', direction='out')
				axMFR.set_xlabel('Cluster index', fontsize=10)
				axMFR.set_ylabel('Mean Firing Rate (Hz)', fontsize=10)
				axMFR.set_ylim((0, dfMFR.max()[0]+3)) # Maximum 3Hz above max firing rate
				heightShift = axMFR.patches[0].get_height()*0.03
				for p in axMFR.patches:
					width = p.get_width()
					axMFR.annotate('~'+str(round(p.get_height(), 3)), (p.get_x()+(width/2), p.get_height() + heightShift), size = 8, ha="center")
				figMFR = axMFR.get_figure()
				figMFR.canvas.manager.window.attributes('-topmost', 1)

				if showMode=='1':
					plt.show()
					plt.close()
				elif showMode=='all':
					pass
				if saveMode==True:
					if not os.path.exists(self.__dir__+'/XtraDataManager/visMFRs'):
						os.makedirs(self.__dir__+'/XtraDataManager/visMFRs')
					figMFRpath = self.__dir__+'/XtraDataManager/visMFRs'+'/MFR'
					for i in unitsListStr:
						figMFRpath+=', '
						figMFRpath+=i
					figMFR.savefig(figMFRpath+'.eps')
					figMFR.savefig(figMFRpath+'.png')
				
				self.dfMFR = dfMFR
				EXIT=True


			if "IFR" in featuresList:
				self.InstFR(binsize = bin_sizeIFR, again=again)
				IFRDic = {}
				for i in unitsList:
					IFRidx = np.where(self.goodUnits==i)[0][0]
					IFRDic[self.IFR[IFRidx][0]] = pd.Series(self.IFR[IFRidx][1:].tolist(), index=np.arange(len(self.IFR[IFRidx][1:]))*(bin_sizeIFR))
				unitsListStr = [str(i) for i in unitsList]

				dfIFR = pd.DataFrame(IFRDic)
				axIFR = dfIFR.plot.area(stacked=False, x_compat=True)
				axIFR.set_xlabel('Recording time (ms)', fontsize=6)
				axIFR.set_ylabel('Instantaneous Firing Rate (y*'+str(round(1./bin_sizeIFR, 3))+'Hz)', fontsize=6)
				axIFR.tick_params(labelsize=6, color='k', direction='out')
				figIFR = axIFR.get_figure()
				figIFR.canvas.manager.window.attributes('-topmost', 1)

				if showMode=='1':
					plt.show()
					plt.close()
				elif showMode=='all':
					pass
				if saveMode==True:
					if not os.path.exists(self.__dir__+'/XtraDataManager/visIFRs'):
						os.makedirs(self.__dir__+'/XtraDataManager/visIFRs')
					figIFRpath = self.__dir__+'/XtraDataManager/visIFRs'+'/IFR'
					unitsListStr = [str(i) for i in unitsList]
					for i in unitsListStr:
						figIFRpath+=', '
						figIFRpath+=i
					figIFR.savefig(figIFRpath+'.eps')
					figIFR.savefig(figIFRpath+'.png')

				
				self.dfIFR = dfIFR
				EXIT=True


			if "CCG" in featuresList:
				if len(unitsList)>5:
					print("/!\ Warning: Display of crosscorrelograms of more than 5 units may not be optimal. Try to plot your figure in several times.")
				
				try:
					assert int(window_sizeCCG/bin_sizeCCG)==int(len(self.CCG[0][0]))-1
					self.CrossCG(bin_size=bin_sizeCCG, window_size=window_sizeCCG, normalize = normalizeCCG, again=again)
				except:
					print("/!\ Warning: Crosscorrelograms were previously calculated with a different binsize or window size \
						than the one(s) you would like to plot the crosscorrelograms with. You can calculate the CCGs with these new values ({} for bins and {} for the window).".format(bin_sizeCCG, window_sizeCCG))
					self.CrossCG(bin_size=bin_sizeCCG, window_size=window_sizeCCG, normalize = normalizeCCG, again=True)
					
				plotsxticks = np.arange(-window_sizeCCG*1000/2, window_sizeCCG*1000/2+bin_sizeCCG*1000, bin_sizeCCG*1000)

				CCGDic_df = {} # Need to create a 3D dataset - panel from pandas. Made from dic of dataframes.
				for i in unitsList:
					CCGDic_sr = {} # Dataframes made from dic of series.
					CCGidxI = np.where(self.units==i)[0][0]
					for j in unitsList:
						CCGidxJ = np.where(self.units==j)[0][0]
						CCGDic_sr[j] = pd.Series(self.CCG[CCGidxI][CCGidxJ].tolist(), index=plotsxticks)
					CCGDic_df[i]=pd.DataFrame(CCGDic_sr)

				pnCCG = pd.Panel(CCGDic_df)
				pnCCG = pnCCG.swapaxes('minor_axis', 'major_axis')
				dfCCG = pnCCG.to_frame()



				figCCG, CCGaxis = plt.subplots(len(unitsList), len(unitsList))
				if len(unitsList)==1:
					colorFlag = 0
					for i, x in enumerate(unitsList):
						for j, y in enumerate(unitsList):
							if i == j:
								if colorFlag%5 == 0:
									color = (51./255, 153./255, 255./255)
								elif colorFlag%5 == 1:
									color = (255./255, 0./255, 0./255)
								elif colorFlag%5 == 2:
									color = (255./255, 255./255, 0./255)
								elif colorFlag%5 == 3:
									color = (0./255, 153./255, 0./255)
								elif colorFlag%5 == 4:
									color = (102./255, 0./255, 204./255)
								colorFlag+=1
							else:
								if matplotlib.rcParams['savefig.facecolor']=='black':
									color = (255./255, 255./255, 255./255) # if matplotlib.style.use('black_background')
								else:
									color = (0./255, 0./255, 0./255) # if not matplotlib.style.use('black_background')
							if plotType == 'bar':
								CCGaxis.bar(plotsxticks,dfCCG[x][y], color = color, edgecolor = color, linewidth=0)
							elif plotType == 'line':
								CCGaxis.plot(plotsxticks,dfCCG[x][y], color = color, edgecolor = color, linewidth=0)
							CCGaxis.set_xlabel('dt (ms)', fontsize=12)
							CCGaxis.set_xlim((-window_sizeCCG*1000/2,window_sizeCCG*1000/2))
							if normalizeCCG:
								CCGaxis.set_ylabel('counts - normalized', fontsize=6)
							else:
								CCGaxis.set_ylabel('counts', fontsize=12)
							CCGaxis.tick_params(labelsize=6, color='k', direction='out')
							CCGaxis.set_title(str(x)+'-'+str(y),fontsize=16)
				else:
					colorFlag = 0	
					for i, x in enumerate(unitsList):
						for j, y in enumerate(unitsList):
							if i == j:
								if colorFlag%5 == 0:
									color = (51./255, 153./255, 255./255)
								elif colorFlag%5 == 1:
									color = (255./255, 0./255, 0./255)
								elif colorFlag%5 == 2:
									color = (255./255, 255./255, 0./255)
								elif colorFlag%5 == 3:
									color = (0./255, 153./255, 0./255)
								elif colorFlag%5 == 4:
									color = (102./255, 0./255, 204./255)
								colorFlag+=1
							else:
								if matplotlib.rcParams['savefig.facecolor']=='black':
									color = (255./255, 255./255, 255./255) # if matplotlib.style.use('black_background')
								else:
									color = (0./255, 0./255, 0./255) # if not matplotlib.style.use('black_background')
							if plotType == 'bar':
								CCGaxis[i][j].bar(plotsxticks,dfCCG[x][y], color = color, edgecolor = color, linewidth=0)
							elif plotType == 'line':
								CCGaxis[i][j].plot(plotsxticks,dfCCG[x][y], color = color)
							CCGaxis[i][j].set_xlabel('dt (ms)', fontsize=6)
							CCGaxis[i][j].set_xlim((-window_sizeCCG*1000/2,window_sizeCCG*1000/2))
							if normalizeCCG:
								CCGaxis[i][j].set_ylabel('counts - normalized', fontsize=6)
							else:
								CCGaxis[i][j].set_ylabel('counts', fontsize=6)
							CCGaxis[i][j].tick_params(labelsize=4, color='k', direction='out')
							CCGaxis[i][j].set_title(str(x)+'-'+str(y),fontsize=8)
				figCCG.tight_layout()
				figCCG.canvas.manager.window.attributes('-topmost', 1)

				if showMode=='1':
					plt.show()
					plt.close()
				elif showMode=='all':
					pass
				if saveMode==True:
					if not os.path.exists(self.__dir__+'/XtraDataManager/visCCGs'):
						os.makedirs(self.__dir__+'/XtraDataManager/visCCGs')
					figCCGpath = self.__dir__+'/XtraDataManager/visCCGs'+'/CCG'
					unitsListStr = [str(i) for i in unitsList]
					for i in unitsListStr:
						figCCGpath+=', '
						figCCGpath+=i
					figCCG.savefig(figCCGpath+'.eps')
					figCCG.savefig(figCCGpath+'.png')

				self.dfCCG = dfCCG
				EXIT=True


			if "ISI" in featuresList:
				if len(unitsList)>5:
					print("/!\ Warning: Display of interspike intervals of more than 5 units may not be optimal. Try to plot your figure in several times.")
				self.InterSI(bin_size=bin_sizeISI, window_size=window_sizeISI, normalize = True, again=again)
				plotsxticks = np.arange(0, window_sizeISI*1000, bin_sizeISI*1000)
				figISI, ISIaxis = plt.subplots(len(unitsList), 1)
				colorFlag = 0
				for i, x in enumerate(unitsList):
					mew, std, skew = self.ISIparamsDic[x][0], self.ISIparamsDic[x][1]**0.5, self.ISIparamsDic[x][2]
					s = '1/mean~rate = %0.1f Hz\nCV = %0.2f \nSkewness = %0.3f' % (1./mew,std/mew,skew)
					if len(unitsList)==1:
						ISIaxis.bar(plotsxticks, self.ISIDic[x], color = (51./255, 153./255, 255./255), linewidth=0.5)
						ISIaxis.annotate(s,xy=(0.65, 0.6),xytext=None,xycoords='axes fraction',fontsize=12, color='k')
						ISIaxis.set_xlim([0,window_sizeISI*1000]) # milliseconds
						ISIaxis.set_xlabel('ISI (ms)', fontsize=12)
						ISIaxis.set_ylabel('Counts', fontsize=12)
						ISIaxis.set_title(str(x),fontsize=16, fontweight='bold')
						ISIaxis.tick_params(labelsize=6, color='k', direction='out')
					else:
						if colorFlag%5 == 0:
							color = (51./255, 153./255, 255./255)
						elif colorFlag%5 == 1:
							color = (255./255, 0./255, 0./255)
						elif colorFlag%5 == 2:
							color = (255./255, 255./255, 0./255)
						elif colorFlag%5 == 3:
							color = (0./255, 153./255, 0./255)
						elif colorFlag%5 == 4:
							color = (102./255, 0./255, 204./255)
						colorFlag+=1
						ISIaxis[i].bar(plotsxticks, self.ISIDic[x], color = color, linewidth=0.5)
						if matplotlib.rcParams['savefig.facecolor']=='black':
							ISIaxis[i].annotate(s,xy=(0.65, 0.6),xytext=None,xycoords='axes fraction',fontsize=6, color='k')
						else:
							ISIaxis[i].annotate(s,xy=(0.65, 0.6),xytext=None,xycoords='axes fraction',fontsize=6, color='k')
						ISIaxis[i].set_xlim([0,window_sizeISI*1000]) # milliseconds
						ISIaxis[i].set_xlabel('ISI (ms)', fontsize=6)
						ISIaxis[i].set_ylabel('Counts', fontsize=6)
						ISIaxis[i].set_title(str(x),fontsize=12, fontweight='bold')
						ISIaxis[i].tick_params(labelsize=6, color='k', direction='out')
				figISI.tight_layout()
				figISI.canvas.manager.window.attributes('-topmost', 1)
				
				if showMode=='1':	
					plt.show()
					plt.close()
				elif showMode=='all':
					pass
				if saveMode==True:
					if not os.path.exists(self.__dir__+'/XtraDataManager/visISIs'):
						os.makedirs(self.__dir__+'/XtraDataManager/visISIs')
					figISIpath = self.__dir__+'/XtraDataManager/visISIs'+'/ISI'
					unitsListStr = [str(i) for i in unitsList]
					for i in unitsListStr:
						figISIpath+=', '
						figISIpath+=i
					figISI.savefig(figISIpath+'.eps')
					figISI.savefig(figISIpath+'.png')
				EXIT=True

			if "WVF" in featuresList:
				self.attribute_spikeTemplates(again=again)
				WVFDic = {unit: self.attributed_spikeTemplatesDic[unit] for unit in unitsList}

				dfWVF = pd.DataFrame(WVFDic)
				axWVF = dfWVF.plot(x_compat=True)
				axWVF.set_ylabel('??', fontsize=10)
				axWVF.set_xlabel('Time samples (30kHz)', fontsize=10)
				axWVF.tick_params(labelsize=6, color='k', direction='out')
				figWVF = axWVF.get_figure()
				figWVF.canvas.manager.window.attributes('-topmost', 1)

				if showMode=='1':
					plt.show()
					plt.close()
				elif showMode=='all':
					pass
				if saveMode==True:
					if not os.path.exists(self.__dir__+'/XtraDataManager/visWVFs'):
						os.makedirs(self.__dir__+'/XtraDataManager/visWVFs')
					figWVFpath = self.__dir__+'/XtraDataManager/visWVFs'+'/WVF'
					unitsListStr = [str(i) for i in unitsList]
					for i in unitsListStr:
						figWVFpath+=', '
						figWVFpath+=i
					figWVF.savefig(figWVFpath+'.eps')
					figWVF.savefig(figWVFpath+'.png')

				self.dfWVF = dfWVF
				EXIT=True

		plt.close() # Prevent figures accumulation in the background

	
	def save_DM(self, filename=None, OBJECT=False, UNPACKED=True, 
		savingList=["sample_rate", "spike_samples", "spike_times", "spike_clusters", "units", "goodUnits", 
		"nanUnits", "usedUnits", "spike_templates", "templates", "attributed_spikeSamples", "attributed_spikeTimes", 
		"attributed_spikeTemplatesIdx", "attributed_spikeTemplates", "IFR", "MFR", "CCG", "ISI", "extractedFeatures"]):
		'''save() -> saving a DataManager() instance. Argument: filename, has to be of the form "xxxxxxx.pkl".'''
		if not os.path.exists(self.__dir__+'/XtraDataManager/save_DManager'):
			os.makedirs(self.__dir__+'/XtraDataManager/save_DManager')

		if filename==None:
			directory = self.__dir__+'/XtraDataManager/save_DManager/'+time.strftime("%Y.%m.%d-%H.%M")
			if not os.path.exists(directory):
				os.makedirs(directory)
			filename=directory+'/DataManager.pkl'
		else:
			filename=directory+'/'+filename

		if OBJECT==True:
			with open(filename, 'wb') as output:
				dill.dump(self, output)

		if UNPACKED==True:
			bar = progressbar.ProgressBar(maxval=19, \
			widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])
			print("Crosscorrelograms calcul in progress...")
			bar.start()
			i=0
			if "sample_rate" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/sample_rate.npy", np.array([self.sample_rate]))
					np.savetxt(directory+"/sample_rate.txt", np.array([self.sample_rate]))
					scipy.io.savemat(directory+"/sample_rate.mat", mdict={'sample_rate': np.asarray(self.sample_rate)})
				except:
					print("No \".sample_rate\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "spike_samples" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/spike_samples.npy", self.spike_samples)
					np.savetxt(directory+"/spike_samples.txt", self.spike_samples)
					scipy.io.savemat(directory+"/spike_samples.mat", mdict={'spike_samples': self.spike_samples})
				except:
					print("No \".spike_samples\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "spike_times" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/spike_times.npy", self.spike_times)
					np.savetxt(directory+"/spike_times.txt", self.spike_times)
					scipy.io.savemat(directory+"/spike_times.mat", mdict={'spike_times': self.spike_times})
				except:
					print("No \".spike_times\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "spike_clusters" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/spike_clusters.npy", self.spike_clusters)
					np.savetxt(directory+"/spike_clusters.txt", self.spike_clusters)
					scipy.io.savemat(directory+"/spike_clusters.mat", mdict={'spike_clusters': self.spike_clusters})
				except:
					print("No \".spike_clusters\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "units" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/units.npy", self.units)
					np.savetxt(directory+"/units.txt", self.units)
					scipy.io.savemat(directory+"/units.mat", mdict={'units': self.units})
				except:
					print("No \".units\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "goodUnits" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/goodUnits.npy", self.goodUnits)
					np.savetxt(directory+"/goodUnits.txt", self.goodUnits)
					scipy.io.savemat(directory+"/goodUnits.mat", mdict={'goodUnits': self.goodUnits})
				except:
					print("No \".goodUnits\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "nanUnits" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/nanUnits.npy", self.nanUnits)
					np.savetxt(directory+"/nanUnits.txt", self.nanUnits)
					scipy.io.savemat(directory+"/nanUnits.mat", mdict={'nanUnits': self.nanUnits})
				except:
					print("No \".nanUnits\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "usedUnits" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/usedUnits.npy", self.usedUnits)
					np.savetxt(directory+"/usedUnits.txt", self.usedUnits)
					scipy.io.savemat(directory+"/usedUnits.mat", mdict={'usedUnits': self.usedUnits})
				except:
					print("No \".usedUnits\" attribute found. It wasn't required by any analysis performed.")
					pass
			if "spike_templates" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/spike_templates.npy", self.spike_templates)
					np.savetxt(directory+"/spike_templates.txt", self.spike_templates)
					scipy.io.savemat(directory+"/spike_templates.mat", mdict={'spike_templates': self.spike_templates})
				except:
					print("No \".spike_templates\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "templates" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/templates.npy", self.templates)
					np.savetxt(directory+"/templates.txt", self.templates)
					scipy.io.savemat(directory+"/templates.mat", mdict={'sample_rate': self.templates})
				except:
					print("No \".templates\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "attributed_spikeSamples" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/attributed_spikeSamples.npy", np.array(self.attributed_spikeSamples))
					with open(directory+"/attributed_spikeSamples.txt", 'wb') as output:
						for array in self.attributed_spikeSamples:
							np.savetxt(output, array)
					scipy.io.savemat(directory+"/attributed_spikeSamples.mat", mdict={'attributed_spikeSamples': np.asarray(self.attributed_spikeSamples)})
				except:
					print("No \".attributed_spikeSamples\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "attributed_spikeTimes" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/attributed_spikeTimes.npy", np.array(self.attributed_spikeTimes))
					with open(directory+"/attributed_spikeTimes.txt", 'wb') as output:
						for array in self.attributed_spikeTimes:
							np.savetxt(output, array)
					scipy.io.savemat(directory+"/attributed_spikeTimes.mat", mdict={'attributed_spikeTimes': np.asarray(self.attributed_spikeTimes)})
				except:
					print("No \".attributed_spikeTimes\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "attributed_spikeTemplatesIdx" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/attributed_spikeTemplatesIdx.npy", np.array(self.attributed_spikeTemplatesIdx))
					with open(directory+"/attributed_spikeTemplatesIdx.txt", 'wb') as output:
						for array in self.attributed_spikeTemplatesIdx:
							np.savetxt(output, array)
					scipy.io.savemat(directory+"/attributed_spikeTemplatesIdx.mat", mdict={'attributed_spikeTemplatesIdx': np.asarray(self.attributed_spikeTemplatesIdx)})
				except:
					print("No \".attributed_spikeTemplatesIdx\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "attributed_spikeTemplates" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/attributed_spikeTemplates.npy", np.array(self.attributed_spikeTemplates))
					with open(directory+"/attributed_spikeTemplates.txt", 'wb') as output:
						for array in self.attributed_spikeTemplates:
							np.savetxt(output, array)
					scipy.io.savemat(directory+"/attributed_spikeTemplates.mat", mdict={'attributed_spikeTemplates': np.asarray(self.attributed_spikeTemplates)})
				except:
					print("No \".attributed_spikeTemplates\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "IFR" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/IFR.npy", self.IFR)
					np.savetxt(directory+"/IFR.txt", self.IFR)
					scipy.io.savemat(directory+"/IFR.mat", mdict={'sample_rate': self.IFR})
				except:
					print("No \".IFR\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "MFR" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/MFR.npy", self.MFR)
					np.savetxt(directory+"/MFR.txt", self.MFR)
					scipy.io.savemat(directory+"/MFR.mat", mdict={'MFR': self.MFR})
				except:
					print("No \".MFR\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "CCG" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/CCG.npy", self.CCG)
					with open(directory+"/CCG.txt", 'wb') as outfile:
					    for i in range(len(self.CCG)):
					        np.savetxt(outfile, self.CCG[i])   
					scipy.io.savemat(directory+"/CCG.mat", mdict={'CCG': self.CCG})
				except:
					print("No \".CCG\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "ISI" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/ISI.npy", self.ISI)
					np.savetxt(directory+"/ISI.txt", self.ISI)
					scipy.io.savemat(directory+"/ISI.mat", mdict={'ISI': self.ISI})
				except:
					print("No \".ISI\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			if "extractedFeatures" in savingList:
				try:
					bar.update(i+1)
					i+=1
					sleep(0.1)
					np.save(directory+"/extractedFeatures.npy", self.extractedFeatures)
					np.savetxt(directory+"/extractedFeatures.txt", self.extractedFeatures)
					scipy.io.savemat(directory+"/extractedFeatures.mat", mdict={'extractedFeatures': self.extractedFeatures})
				except:
					print("No \".extractedFeatures\" attribute found. It wasn't required by any analysis previously performed.")
					pass
			bar.finish()
			print("DataManager attributes unpacked and saved as numpy, text and matlab files.")

	def compute_all_default_and_save(self):
		self.extractFeatures()
		self.InstFR()
		self.attribute_spikeTemplates()
		self.save_DM()





if __name__ == '__main__':
	data = DataManager()

'''
CCGCCG = np.zeros((505, 81))
for i in range(len(data.CCG)):
     for j in range(len(data.CCG[i])):
         if i==j:
             CCGCCG[i]+=data.CCG[i][j]
np.unique(np.where(np.isnan(CCGCCG))[0])'''

'''
import numpy as np
import matplotlib.pyplot as plt
import FeaturesExtraction as fext
data = fext.DataManager(2)
'''

''' -->> Fit ISI <<--
plt.close()
y = np.array(data.ISIListDic[4])*1000
x = np.arange(y.shape[0])
h = plt.hist(y, bins=range(400), color='w')

dists = ['alpha', 'lognorm']
for dist_name in dists:
	dist = getattr(scipy.stats, dist_name)
	param = dist.fit(y)
	pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1]) * y.shape[0]
	plt.plot(pdf_fitted, label=dist, legend=True)

plt.xlim(0,400)
plt.show()

'''
