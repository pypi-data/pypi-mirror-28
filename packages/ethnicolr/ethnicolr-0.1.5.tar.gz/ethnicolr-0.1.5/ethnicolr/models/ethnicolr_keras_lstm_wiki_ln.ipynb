{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_first</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Asian,GreaterEastAsian,EastAsian</th>\n",
       "      <td>5497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian,GreaterEastAsian,Japanese</th>\n",
       "      <td>7334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian,IndianSubContinent</th>\n",
       "      <td>7861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterAfrican,Africans</th>\n",
       "      <td>3672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterAfrican,Muslim</th>\n",
       "      <td>6242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,British</th>\n",
       "      <td>41445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,EastEuropean</th>\n",
       "      <td>8329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,Jewish</th>\n",
       "      <td>10239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,French</th>\n",
       "      <td>12293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Germanic</th>\n",
       "      <td>3869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Hispanic</th>\n",
       "      <td>10412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Italian</th>\n",
       "      <td>11867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Nordic</th>\n",
       "      <td>4813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       name_first\n",
       "race                                             \n",
       "Asian,GreaterEastAsian,EastAsian             5497\n",
       "Asian,GreaterEastAsian,Japanese              7334\n",
       "Asian,IndianSubContinent                     7861\n",
       "GreaterAfrican,Africans                      3672\n",
       "GreaterAfrican,Muslim                        6242\n",
       "GreaterEuropean,British                     41445\n",
       "GreaterEuropean,EastEuropean                 8329\n",
       "GreaterEuropean,Jewish                      10239\n",
       "GreaterEuropean,WestEuropean,French         12293\n",
       "GreaterEuropean,WestEuropean,Germanic        3869\n",
       "GreaterEuropean,WestEuropean,Hispanic       10412\n",
       "GreaterEuropean,WestEuropean,Italian        11867\n",
       "GreaterEuropean,WestEuropean,Nordic          4813"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "NGRAMS = 2\n",
    "EPOCHS = 15\n",
    "\n",
    "# Wikilabels\n",
    "df = pd.read_csv('../data/wiki/wiki_name_race.csv')\n",
    "df.dropna(subset=['name_first', 'name_last'], inplace=True)\n",
    "sdf = df\n",
    "\n",
    "# Additional features\n",
    "sdf['name_first'] = sdf.name_first.str.title()\n",
    "\n",
    "sdf.groupby('race').agg({'name_first': 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words = 1946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python2.7/site-packages/ipykernel/__main__.py:28: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max feature len = 71, Avg. feature len = 5\n"
     ]
    }
   ],
   "source": [
    "# only last name will be use to train the model\n",
    "sdf['name_last_name_first'] = sdf['name_last'] \n",
    "\n",
    "# build n-gram list\n",
    "vect = CountVectorizer(analyzer='char', max_df=0.3, min_df=3, ngram_range=(NGRAMS, NGRAMS), lowercase=False) \n",
    "a = vect.fit_transform(sdf.name_last_name_first)\n",
    "vocab = vect.vocabulary_\n",
    "\n",
    "# sort n-gram by freq (highest -> lowest)\n",
    "words = []\n",
    "for b in vocab:\n",
    "    c = vocab[b]\n",
    "    #print(b, c, a[:, c].sum())\n",
    "    words.append((a[:, c].sum(), b))\n",
    "    #break\n",
    "words = sorted(words, reverse=True)\n",
    "words_list = [w[1] for w in words]\n",
    "num_words = len(words_list)\n",
    "print(\"num_words = %d\" % num_words)\n",
    "\n",
    "\n",
    "def find_ngrams(text, n):\n",
    "    a = zip(*[text[i:] for i in range(n)])\n",
    "    wi = []\n",
    "    for i in a:\n",
    "        w = ''.join(i)\n",
    "        try:\n",
    "            idx = words_list.index(w)\n",
    "        except:\n",
    "            idx = 0\n",
    "        wi.append(idx)\n",
    "    return wi\n",
    "\n",
    "# build X from index of n-gram sequence\n",
    "X = np.array(sdf.name_last_name_first.apply(lambda c: find_ngrams(c, NGRAMS)))\n",
    "\n",
    "# check max/avg feature\n",
    "X_len = []\n",
    "for x in X:\n",
    "    X_len.append(len(x))\n",
    "\n",
    "max_feature_len = max(X_len)\n",
    "avg_feature_len = int(np.mean(X_len))\n",
    "\n",
    "print(\"Max feature len = %d, Avg. feature len = %d\" % (max_feature_len, avg_feature_len))\n",
    "y = np.array(sdf.race.astype('category').cat.codes)\n",
    "\n",
    "# Split train and test dataset\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train a LSTM model\n",
    "\n",
    "ref: http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107098 train sequences\n",
      "26775 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (107098, 20)\n",
      "X_test shape: (26775, 20)\n",
      "13 classes\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (107098, 13)\n",
      "y_test shape: (26775, 13)\n"
     ]
    }
   ],
   "source": [
    "'''The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "Notes:\n",
    "\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "\n",
    "max_features = num_words # 20000\n",
    "feature_len = 20 # avg_feature_len # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=feature_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=feature_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 32)            62272     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 146,381.0\n",
      "Trainable params: 146,381\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 32, input_length=feature_len))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 96388 samples, validate on 10710 samples\n",
      "Epoch 1/15\n",
      "141s - loss: 1.6834 - acc: 0.4915 - val_loss: 1.4706 - val_acc: 0.5553\n",
      "Epoch 2/15\n",
      "139s - loss: 1.4253 - acc: 0.5746 - val_loss: 1.3817 - val_acc: 0.5919\n",
      "Epoch 3/15\n",
      "133s - loss: 1.3492 - acc: 0.6009 - val_loss: 1.3427 - val_acc: 0.6020\n",
      "Epoch 4/15\n",
      "149s - loss: 1.2990 - acc: 0.6174 - val_loss: 1.3053 - val_acc: 0.6162\n",
      "Epoch 5/15\n",
      "146s - loss: 1.2673 - acc: 0.6282 - val_loss: 1.2870 - val_acc: 0.6257\n",
      "Epoch 6/15\n",
      "149s - loss: 1.2412 - acc: 0.6346 - val_loss: 1.2748 - val_acc: 0.6282\n",
      "Epoch 7/15\n",
      "150s - loss: 1.2197 - acc: 0.6415 - val_loss: 1.2579 - val_acc: 0.6370\n",
      "Epoch 8/15\n",
      "149s - loss: 1.2004 - acc: 0.6455 - val_loss: 1.2527 - val_acc: 0.6402\n",
      "Epoch 9/15\n",
      "142s - loss: 1.1855 - acc: 0.6522 - val_loss: 1.2406 - val_acc: 0.6440\n",
      "Epoch 10/15\n",
      "158s - loss: 1.1721 - acc: 0.6549 - val_loss: 1.2411 - val_acc: 0.6414\n",
      "Epoch 11/15\n",
      "139s - loss: 1.1591 - acc: 0.6582 - val_loss: 1.2351 - val_acc: 0.6458\n",
      "Epoch 12/15\n",
      "140s - loss: 1.1484 - acc: 0.6623 - val_loss: 1.2288 - val_acc: 0.6511\n",
      "Epoch 13/15\n",
      "157s - loss: 1.1366 - acc: 0.6656 - val_loss: 1.2249 - val_acc: 0.6516\n",
      "Epoch 14/15\n",
      "151s - loss: 1.1262 - acc: 0.6676 - val_loss: 1.2216 - val_acc: 0.6539\n",
      "Epoch 15/15\n",
      "158s - loss: 1.1164 - acc: 0.6707 - val_loss: 1.2243 - val_acc: 0.6524\n",
      "Test score: 1.20667794086\n",
      "Test accuracy: 0.654939309077\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=EPOCHS,\n",
    "          validation_split=0.1, verbose=2)\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size, verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "     Asian,GreaterEastAsian,EastAsian       0.76      0.74      0.75      1099\n",
      "      Asian,GreaterEastAsian,Japanese       0.82      0.84      0.83      1467\n",
      "             Asian,IndianSubContinent       0.65      0.67      0.66      1572\n",
      "              GreaterAfrican,Africans       0.50      0.36      0.42       734\n",
      "                GreaterAfrican,Muslim       0.56      0.52      0.54      1248\n",
      "              GreaterEuropean,British       0.72      0.85      0.78      8289\n",
      "         GreaterEuropean,EastEuropean       0.75      0.62      0.68      1666\n",
      "               GreaterEuropean,Jewish       0.43      0.37      0.40      2048\n",
      "  GreaterEuropean,WestEuropean,French       0.56      0.47      0.51      2459\n",
      "GreaterEuropean,WestEuropean,Germanic       0.38      0.29      0.33       774\n",
      "GreaterEuropean,WestEuropean,Hispanic       0.63      0.49      0.55      2082\n",
      " GreaterEuropean,WestEuropean,Italian       0.61      0.74      0.67      2374\n",
      "  GreaterEuropean,WestEuropean,Nordic       0.65      0.53      0.58       963\n",
      "\n",
      "                          avg / total       0.65      0.65      0.65     26775\n",
      "\n",
      "[[ 814   49   15    6   15  136   10   11   13    4   11   11    4]\n",
      " [  30 1233    9   15   14   50    6   10    9    2   46   36    7]\n",
      " [  16   15 1055   29  134  153   11   33   24    8   28   55   11]\n",
      " [  15   54   57  267   69  110   10   28   29    6   34   48    7]\n",
      " [  20   22  132   43  645  115   45   77   44    8   35   46   16]\n",
      " [  57   27   85   46   62 7076   51  276  294   60   62  130   63]\n",
      " [  20   17   38   13   40  136 1029  156   53   43   28   62   31]\n",
      " [  13   15   56   17   81  634   95  754  103   87   71   93   29]\n",
      " [  24   18   48   41   29  576   32  117 1160   53  135  201   25]\n",
      " [  11    1   16    5    8  206   14  142   54  223   13   31   50]\n",
      " [  12   36   56   22   25  244   22   65  133   39 1018  391   19]\n",
      " [  29   11   32   25   12  178   34   37  120   21  112 1751   12]\n",
      " [  16    2   13    4   16  205   19   62   35   40   20   20  511]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test, verbose=2)\n",
    "p = model.predict_proba(X_test, verbose=2) # to predict probability\n",
    "target_names = list(sdf.race.astype('category').cat.categories)\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.models.Sequential object at 0x7f4fb10d0ed0>\n"
     ]
    }
   ],
   "source": [
    "model.save('./wiki/lstm/wiki_ln_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(words_list, columns=['vocab'])\n",
    "words_df.to_csv('./wiki/lstm/wiki_ln_vocab.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
